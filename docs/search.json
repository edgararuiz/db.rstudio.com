[
  {
    "href": "advanced/snippets.html#snippet-files",
    "title": "RStudio Connections",
    "section": "Snippet Files",
    "text": "A Connection Snippet File is an R code snippet with additional metadata which is intended to initialize a connection. This file can be as simple as:\nlibrary(readr)\ndata <- read_csv(readr_example(\"mtcars.csv\"))\nOnce this file is saved under /etc/rstudio/connections/ as Motor Trend Cars.R, RStudio will make this connection as available as:\n\nThe path is configurable through the connections-path environment variable and multiple connection files can be specified.\nIn order to parameterize this connection, one can create fields using using the ${Position:Label=Default} syntax:\n\nPosition: The row position starting at zero.\nLabel: The label assigned to this field.\nDefault: An optional default value.\n\nFor example, we can filter out this dataframe to produce the following connection interface:\nlibrary(readr)\ndata <- read_csv(readr_example(\"mtcars.csv\"))\ndata[data$mpg == ${0:Miles per Gallon=21.4} | data$cyl == ${1:Cylinders=6}, ]\n\nIn order to create a ; separated list of values, one can use the syntax ${Position:Label=Default:Key}. Semicolon-separated list are common in database connections and therefore, natively supported in snippet files, for instance:\n\"${2:Letters=ABC:LettersKey}${3:Numbers=123:NumbersKey}\"\n\nThere are a couple escape characters supported: $colon$ to escape : and $equal to escape =."
  },
  {
    "href": "advanced/snippets.html#r-packages",
    "title": "RStudio Connections",
    "section": "R Packages",
    "text": "Package Structure\nA package supporting connections defines the following components:\n\nConnections File:  A DCF file must be created under inst/rstudio/connections.dcf to enumerate each connection supported in the package.\nSnippet Files: Snippet files are stored under inst/rstudio/connections/.\n\nAs a quick start, the Rstudio Connections Example GitHub repo contains a working example of this structure.\n\n\nConnections Contract\nYou can integrate with RStudio’s Connection Pane to allow users to explore connections created with your R package by using the Connections Contract.\n\n\nSnippet Files\nSnippet Files are specified under the /inst/rstudio/connections and follow the same syntax mentioned in the “Snippet Files” section.\n\n\nShiny Application\nFor advanced connection interfaces, a shiny application can be specified. See sparklyr for a working application."
  },
  {
    "href": "advanced/translation.html#vectors",
    "title": "SQL translation",
    "section": "Vectors",
    "text": "Most filtering, mutating, or summarising operations only perform simple mathematical operations. These operations are very similar between R and SQL, so they’re easy to translate. To see what’s happening yourself, you can use translate_sql(). The basic techniques that underlie the implementation of translate_sql() are described in “Advanced R”. translate_sql() is built on top of R’s parsing engine and has been carefully designed to generate correct SQL. It also protects you against SQL injection attacks by correctly escaping the strings and variable names needed by the database that you’re connecting to.\nThe following examples work through some of the basic differences between R and SQL.\n\n\" and ' mean different things\n\n::: {.cell indent=” “}\n```{.r .cell-code}\n# In SQLite, variable names are escaped by double quotes:\ntranslate_sql(x)\n```\n\n::: {.cell-output-stdout}\n```\n#> <SQL> `x`\n```\n:::\n\n```{.r .cell-code}\n# And strings are escaped by single quotes\ntranslate_sql(\"x\")\n```\n\n::: {.cell-output-stdout}\n```\n#> <SQL> 'x'\n```\n:::\n:::\n\nMany functions have slightly different names\n\n::: {.cell indent=” “}\n```{.r .cell-code}\ntranslate_sql(x == 1 && (y < 2 || z > 3))\n```\n\n::: {.cell-output-stdout}\n```\n#> <SQL> `x` = 1.0 AND (`y` < 2.0 OR `z` > 3.0)\n```\n:::\n\n```{.r .cell-code}\ntranslate_sql(x ^ 2 < 10)\n```\n\n::: {.cell-output-stdout}\n```\n#> <SQL> POWER(`x`, 2.0) < 10.0\n```\n:::\n\n```{.r .cell-code}\ntranslate_sql(x %% 2 == 10)\n```\n\n::: {.cell-output-stdout}\n```\n#> <SQL> `x` % 2.0 = 10.0\n```\n:::\n::: * And some functions have different argument orders:\n::: {.cell indent=” “}\n```{.r .cell-code}\ntranslate_sql(substr(x, 5, 10))\n```\n\n::: {.cell-output-stdout}\n```\n#> <SQL> SUBSTR(`x`, 5, 6)\n```\n:::\n\n```{.r .cell-code}\ntranslate_sql(log(x, 10))\n```\n\n::: {.cell-output-stdout}\n```\n#> <SQL> LOG(10.0, `x`)\n```\n:::\n:::\n\nR and SQL have different defaults for integers and reals. In R, 1 is a real, and 1L is an integer. In SQL, 1 is an integer, and 1.0 is a real\n\n::: {.cell indent=” “}\n```{.r .cell-code}\ntranslate_sql(1)\n```\n\n::: {.cell-output-stdout}\n```\n#> <SQL> 1.0\n```\n:::\n\n```{.r .cell-code}\ntranslate_sql(1L)\n```\n\n::: {.cell-output-stdout}\n```\n#> <SQL> 1\n```\n:::\n:::\n\nIf statements are translated into a case statement:\n\n::: {.cell indent=” “}\n```{.r .cell-code}\ntranslate_sql(if (x > 5) \"big\" else \"small\")\n```\n\n::: {.cell-output-stdout}\n```\n#> <SQL> CASE WHEN (`x` > 5.0) THEN ('big') WHEN NOT(`x` > 5.0) THEN ('small') END\n```\n:::\n:::\n\nKnown functions\ndplyr knows how to convert the following R functions to SQL:\n\nbasic math operators: +, -, *, /, %%, ^\nmath functions: abs, acos, acosh, asin, asinh, atan, atan2, atanh, ceiling, cos, cosh, cot, coth, exp, floor, log, log10, round, sign, sin, sinh, sqrt, tan, tanh\nlogical comparisons: <, <=, !=, >=, >, ==, %in%\nboolean operations: &, &&, |, ||, !, xor\nbasic aggregations: mean, sum, min, max, sd, var\nstring functions: tolower, toupper, trimws, nchar, substr\ncoerce types: as.numeric, as.integer, as.character\n\nPerfect translation is not possible because databases don’t have all the functions that R does. The goal of dplyr is to provide a semantic rather than a literal translation: what you mean rather than what is done. In fact, even for functions that exist both in databases and R, you shouldn’t expect results to be identical; database programmers have different priorities than R core programmers. For example, in R, in order to get a higher level of numerical accuracy, mean() loops through the data twice. R’s mean() also provides a trim option for computing trimmed means; this is something that databases do not provide. Databases automatically drop NULLs (their equivalent of missing values), whereas in R you have to ask nicely. This means the essence of simple calls like mean(x) will be translated accurately, but more complicated calls like mean(x, trim = 0.5, na.rm = TRUE) will raise an error:\n\ntranslate_sql(mean(x, na.rm = TRUE))\n\n#> <SQL> AVG(`x`) OVER ()\n\ntranslate_sql(mean(x, trim = 0.1))\n\n#> Error in mean(x, trim = 0.1): unused argument (trim = 0.1)\n\n\ntranslate_sql() takes an optional con parameter. If not supplied, this causes dplyr to generate (approximately) SQL-92-compliant SQL. If supplied, dplyr uses sql_translate_env() to look up a custom environment, which makes it possible for different databases to generate slightly different SQL; see vignette(\"new-backend\") for more details.\n\n\nUnknown functions\nAny function that dplyr doesn’t know how to convert is left as-is. This means that database functions that are not covered by dplyr can be used directly via translate_sql(). Here a couple of examples that will work with SQLite:\n\ntranslate_sql(glob(x, y))\n\n#> <SQL> glob(`x`, `y`)\n\ntranslate_sql(x %like% \"ab%\")\n\n#> <SQL> `x` like 'ab%'\n\n\n\n\nWindow functions\nThings get a little trickier with window functions, because SQL’s window functions are considerably more expressive than the specific variants provided by base R or dplyr. They have the form [expression] OVER ([partition clause] [order clause] [frame_clause]):\n\nThe expression is a combination of variable names and window functions. Support for window functions varies from database to database, but most support the ranking functions, lead, lag, nth, first, last, count, min, max, sum, avg and stddev.\nThe partition clause specifies how the window function is broken down over groups. It plays an analogous role to GROUP BY for aggregate functions, and group_by() in dplyr. It is possible for different window functions to be partitioned into different groups, but not all databases support it, and neither does dplyr.\nThe order clause controls the ordering (when it makes a difference). This is important for the ranking functions since it specifies which variables to rank by, but it’s also needed for cumulative functions and lead. Whenever you’re thinking about before and after in SQL, you must always tell it which variable defines the order. If the order clause is missing when needed, some databases fail with an error message while others return non-deterministic results.\nThe frame clause defines which rows, or frame, that are passed to the window function, describing which rows (relative to the current row) should be included. The frame clause provides two offsets which determine the start and end of frame. There are three special values: -Inf means to include all preceeding rows (in SQL, “unbounded preceding”), 0 means the current row (“current row”), and Inf means all following rows (“unbounded following)”. The complete set of options is comprehensive, but fairly confusing, and is summarised visually below.\nOf the many possible specifications, there are only three that commonly used. They select between aggregation variants:\n\nRecycled: BETWEEN UNBOUND PRECEEDING AND UNBOUND FOLLOWING\nCumulative: BETWEEN UNBOUND PRECEEDING AND CURRENT ROW\nRolling: BETWEEN 2 PRECEEDING AND 2 FOLLOWING\n\ndplyr generates the frame clause based on whether your using a recycled aggregate or a cumulative aggregate.\n\nTo see how individual window functions are translated to SQL, we can again use translate_sql():\n\ntranslate_sql(mean(G))\n\n#> Warning: Missing values are always removed in SQL.\n#> Use `AVG(x, na.rm = TRUE)` to silence this warning\n#> This warning is displayed only once per session.\n\n\n#> <SQL> AVG(`G`) OVER ()\n\ntranslate_sql(rank(G))\n\n#> <SQL> RANK() OVER (ORDER BY `G`)\n\ntranslate_sql(ntile(G, 2))\n\n#> <SQL> NTILE(2) OVER (ORDER BY `G`)\n\ntranslate_sql(lag(G))\n\n#> <SQL> LAG(`G`, 1, NULL) OVER ()\n\n\nIf the tbl has been grouped or arranged previously in the pipeline, then dplyr will use that information to set the “partition by” and “order by” clauses. For interactive exploration, you can achieve the same effect by setting the vars_group and vars_order arguments to translate_sql()\n\ntranslate_sql(cummean(G), vars_order = \"year\")\n\n#> <SQL> AVG(`G`) OVER (ORDER BY `year` ROWS UNBOUNDED PRECEDING)\n\ntranslate_sql(rank(), vars_group = \"ID\")\n\n#> <SQL> RANK() OVER (PARTITION BY `ID`)\n\n\nThere are some challenges when translating window functions between R and SQL, because dplyr tries to keep the window functions as similar as possible to both the existing R analogues and to the SQL functions. This means that there are three ways to control the order clause depending on which window function you’re using:\n\nFor ranking functions, the ordering variable is the first argument: rank(x), ntile(y, 2). If omitted or NULL, will use the default ordering associated with the tbl (as set by arrange()).\nAccumulating aggregates only take a single argument (the vector to aggregate). To control ordering, use order_by().\nAggregates implemented in dplyr (lead, lag, nth_value, first_value, last_value) have an order_by argument. Supply it to override the default ordering.\n\nThe three options are illustrated in the snippet below:\n\nmutate(players,\n  min_rank(yearID),\n  order_by(yearID, cumsum(G)),\n  lead(G, order_by = yearID)\n)\n\nCurrently there is no way to order by multiple variables, except by setting the default ordering with arrange(). This will be added in a future release."
  },
  {
    "href": "advanced/translation.html#whole-tables",
    "title": "SQL translation",
    "section": "Whole tables",
    "text": "All dplyr verbs generate a SELECT statement. To demonstrate, we’ll make a temporary database with a couple of tables:\n\ncon <- DBI::dbConnect(RSQLite::SQLite(), \":memory:\")\nflights <- copy_to(con, nycflights13::flights)\nairports <- copy_to(con, nycflights13::airports)\n\n\nSingle table verbs\n\nselect() and mutate() modify the SELECT clause:\n\n::: {.cell indent=” “}\n```{.r .cell-code}\nflights %>%\n  select(contains(\"delay\")) %>%\n  show_query()\n```\n\n::: {.cell-output-stdout}\n```\n#> <SQL>\n#> SELECT `dep_delay`, `arr_delay`\n#> FROM `nycflights13::flights`\n```\n:::\n\n```{.r .cell-code}\n\nflights %>%\n  select(distance, air_time) %>%  \n  mutate(speed = distance / (air_time / 60)) %>%\n  show_query()\n```\n\n::: {.cell-output-stdout}\n```\n#> <SQL>\n#> SELECT `distance`, `air_time`, `distance` / (`air_time` / 60.0) AS `speed`\n#> FROM `nycflights13::flights`\n```\n:::\n:::\n(As you can see here, the generated SQL isn't always as minimal as you\nmight generate by hand.)\n\nfilter() generates a WHERE clause:\n\n::: {.cell indent=” “}\n```{.r .cell-code}\nflights %>% \n  filter(month == 1, day == 1) %>%\n  show_query()\n```\n\n::: {.cell-output-stdout}\n```\n#> <SQL>\n#> SELECT *\n#> FROM `nycflights13::flights`\n#> WHERE ((`month` = 1.0) AND (`day` = 1.0))\n```\n:::\n:::\n\narrange() generates an ORDER BY clause:\n\n::: {.cell indent=” “}\n```{.r .cell-code}\nflights %>% \n  arrange(carrier, desc(arr_delay)) %>%\n  show_query()\n```\n\n::: {.cell-output-stdout}\n```\n#> <SQL>\n#> SELECT *\n#> FROM `nycflights13::flights`\n#> ORDER BY `carrier`, `arr_delay` DESC\n```\n:::\n:::\n\nsummarise() and group_by() work together to generate a GROUP BY clause:\n\n::: {.cell indent=” “}\n```{.r .cell-code}\nflights %>%\n  group_by(month, day) %>%\n  summarise(delay = mean(dep_delay)) %>%\n  show_query()\n```\n\n::: {.cell-output-stderr}\n```\n#> Warning: Missing values are always removed in SQL.\n#> Use `mean(x, na.rm = TRUE)` to silence this warning\n#> This warning is displayed only once per session.\n```\n:::\n\n::: {.cell-output-stderr}\n```\n#> `summarise()` has grouped output by 'month'. You can override using the `.groups` argument.\n```\n:::\n\n::: {.cell-output-stdout}\n```\n#> <SQL>\n#> SELECT `month`, `day`, AVG(`dep_delay`) AS `delay`\n#> FROM `nycflights13::flights`\n#> GROUP BY `month`, `day`\n```\n:::\n:::\n\n\nDual table verbs\ninner_join() | SELECT * FROM x JOIN y ON x.a = y.a\nleft_join() | SELECT * FROM x LEFT JOIN y ON x.a = y.a\nright_join() | SELECT * FROM x RIGHT JOIN y ON x.a = y.a\nfull_join() | SELECT * FROM x FULL JOIN y ON x.a = y.a\nsemi_join() | SELECT * FROM x WHERE EXISTS (SELECT 1 FROM y WHERE x.a = y.a)\nanti_join() | SELECT * FROM x WHERE NOT EXISTS (SELECT 1 FROM y WHERE x.a = y.a)\nintersect(x, y)| SELECT * FROM x INTERSECT SELECT * FROM y\nunion(x, y) | SELECT * FROM x UNION SELECT * FROM y\nsetdiff(x, y) | SELECT * FROM x EXCEPT SELECT * FROM y\nx and y don’t have to be tables in the same database. If you specify copy = TRUE, dplyr will copy the y table into the same location as the x variable. This is useful if you’ve downloaded a summarised dataset and determined a subset of interest that you now want the full data for. You can use semi_join(x, y, copy = TRUE) to upload the indices of interest to a temporary table in the same database as x, and then perform a efficient semi join in the database.\nIf you’re working with large data, it maybe also be helpful to set auto_index = TRUE. That will automatically add an index on the join variables to the temporary table.\n\n\nBehind the scenes\nThe verb level SQL translation is implemented on top of tbl_lazy, which basically tracks the operations you perform in a pipeline (see lazy-ops.R). Turning that into a SQL query takes place in three steps:\n\nsql_build() recurses over the lazy op data structure building up query objects (select_query(), join_query(), set_op_query(), etc.) that represent the different subtypes of SELECT queries that we might generate.\nsql_optimise() takes a pass over these SQL objects, looking for potential optimisations. Currently this only involves removing subqueries where possible.\nsql_render() calls an SQL generation function (sql_select(), sql_join(), sql_subquery(), sql_semijoin(), etc.) to produce the actual SQL. Each of these functions is a generic, taking the connection as an argument, so that the details can be customised for different databases."
  },
  {
    "href": "advanced/backend.html#getting-started",
    "title": "Implementing a new backend",
    "section": "Getting started",
    "text": "Start by creating a package. It’s up to you what to call the package, but following the existing pattern of RSQLite, RMySQL, RPostgres and ROracle will make it easier for people to find it. For this example, I’ll call my package RKazam.\nIn your DESCRIPTION, make sure to include:\nImports:\n  DBI (>= 0.3.0),\n  methods\nSuggests:\n  DBItest, testthat\nImporting DBI is fine, because your users are not supposed to attach your package anyway; the preferred method is to attach DBI and use explicit qualification via :: to access the driver in your package (which needs to be done only once)."
  },
  {
    "href": "advanced/backend.html#testing",
    "title": "Implementing a new backend",
    "section": "Testing",
    "text": "Why test at this early stage? Because testing should be an integral part of the software development cycle. Test right from the start, add automated tests as you go, and finish faster (because tests are automated) while maintaining superb code quality (because tests also check corner cases that you might not be aware of). Don’t worry: if some test cases are difficult or impossible to satisfy, or take too long to run, you can just turn them off.\nTake the time now to head over to the DBItest vignette. You will find a vast amount of ready-to-use test cases that will help you in the process of implementing your new DBI backend.\nvignette(\"test\", package = \"DBItest\")\nAdd custom tests that are not covered by DBItest at your discretion, or enhance DBItest and file a pull request if the test is generic enough to be useful for many DBI backends."
  },
  {
    "href": "advanced/backend.html#driver",
    "title": "Implementing a new backend",
    "section": "Driver",
    "text": "Start by making a driver class which inherits from DBIDriver. This class doesn’t need to do anything; it is just used to dispatch other generics to the right method. Users don’t need to know about this, so you can remove it from the default help listing with @keywords internal:\n\n#' Driver for Kazam database.\n#' \n#' @keywords internal\n#' @export\n#' @import DBI\n#' @import methods\nsetClass(\"KazamDriver\", contains = \"DBIDriver\")\n\nThe driver class was more important in older versions of DBI, so you should also provide a dummy dbUnloadDriver() method:\n\n#' @export\n#' @rdname Kazam-class\nsetMethod(\"dbUnloadDriver\", \"KazamDriver\", function(drv, ...) {\n  TRUE\n})\n\nIf your package needs global setup or tear down, do this in the .onLoad() and .onUnload() functions.\nYou might also want to add a show() method so the object prints nicely:\n\nsetMethod(\"show\", \"KazamDriver\", function(object) {\n  cat(\"<KazamDriver>\\n\")\n})\n\nNext, create Kazam(), which instantiates this class:\n\n#' @export\nKazam <- function() {\n  new(\"KazamDriver\")\n}\n\nKazam()\n\n#> <KazamDriver>"
  },
  {
    "href": "advanced/backend.html#connection",
    "title": "Implementing a new backend",
    "section": "Connection",
    "text": "Next, create a connection class that inherits from DBIConnection. This should store all the information needed to connect to the database. If you’re talking to a C API, this will include a slot that holds an external pointer.\n\n#' Kazam connection class.\n#' \n#' @export\n#' @keywords internal\nsetClass(\"KazamConnection\", \n  contains = \"DBIConnection\", \n  slots = list(\n    host = \"character\", \n    username = \"character\", \n    # and so on\n    ptr = \"externalptr\"\n  )\n)\n\nNow that you have some of the boilerplate out of the way, you can start work on the connection. The most important method here is dbConnect(), which allows you to connect to a specified instance of the database. Note the use of @rdname Kazam. This ensures that Kazam() and the connect method are documented together.\n\n#' @param drv An object created by \\code{Kazam()} \n#' @rdname Kazam\n#' @export\n#' @examples\n#' \\dontrun{\n#' db <- dbConnect(RKazam::Kazam())\n#' dbWriteTable(db, \"mtcars\", mtcars)\n#' dbGetQuery(db, \"SELECT * FROM mtcars WHERE cyl == 4\")\n#' }\nsetMethod(\"dbConnect\", \"KazamDriver\", function(drv, ...) {\n  # ...\n  \n  new(\"KazamConnection\", host = host, ...)\n})\n\n\nReplace ... with the arguments needed to connect to your database. You’ll always need to include ... in the arguments, even if you don’t use it, for compatibility with the generic.\nThis is likely to be where people first come for help, so the examples should show how to connect to the database, and how to query it. (Obviously these examples won’t work yet.) Ideally, include examples that can be run right away (perhaps relying on a publicly hosted database), but failing that, surround it in \\dontrun{} so people can at least see the code.\n\nNext, implement show() and dbDisconnect() methods."
  },
  {
    "href": "advanced/backend.html#results",
    "title": "Implementing a new backend",
    "section": "Results",
    "text": "Finally, you’re ready to implement the meat of the system: fetching results of a query into a data frame. First define a results class:\n\n#' Kazam results class.\n#' \n#' @keywords internal\n#' @export\nsetClass(\"KazamResult\", \n  contains = \"DBIResult\",\n  slots = list(ptr = \"externalptr\")\n)\n\nThen write a dbSendQuery() method. This takes a connection and SQL string as arguments, and returns a result object. Again ... is needed for compatibility with the generic, but you can add other arguments if you need them.\n\n#' Send a query to Kazam.\n#' \n#' @export\n#' @examples \n#' # This is another good place to put examples\nsetMethod(\"dbSendQuery\", \"KazamConnection\", function(conn, statement, ...) {\n  # some code\n  new(\"KazamResult\", ...)\n})\n\nNext, implement dbClearResult(), which should close the result set and free all resources associated with it:\n\n#' @export\nsetMethod(\"dbClearResult\", \"KazamResult\", function(res, ...) {\n  # free resources\n  TRUE\n})\n\nThe hardest part of every DBI package is writing the dbFetch() method. This needs to take a result set and (optionally) the number of records to return, and create a dataframe. Mapping R’s data types to those of your database may require a custom implementation of the dbDataType() method for your connection class:\n\n#' Retrieve records from Kazam query\n#' @export\nsetMethod(\"dbFetch\", \"KazamResult\", function(res, n = -1, ...) {\n  ...\n})\n\n# (optionally)\n\n#' Find the database data type associated with an R object\n#' @export\nsetMethod(\"dbDataType\", \"KazamConnection\", function(dbObj, obj, ...) {\n  ...\n})\n\nNext, implement dbHasCompleted(), which should return a logical indicating if there are any rows remaining to be fetched.\n\n#' @export\nsetMethod(\"dbHasCompleted\", \"KazamResult\", function(res, ...) { \n  \n})\n\nWith these four methods in place, you can now use the default dbGetQuery() to send a query to the database, retrieve results if available, and then clean up. Spend some time now making sure this works with an existing database, or relax and let the DBItest package do the work for you."
  },
  {
    "href": "advanced/backend.html#sql-methods",
    "title": "Implementing a new backend",
    "section": "SQL methods",
    "text": "You’re now in the home stretch, and can make your wrapper substantially more useful by implementing methods that wrap around variations in SQL across databases:\n\ndbQuoteString() and dbQuoteIdentifer() are used to safely quote strings and identifiers to avoid SQL injection attacks. Note that the former must be vectorized, but not the latter.\ndbWriteTable() creates a database table given an R dataframe. I’d recommend using the functions prefixed with sql in this package to generate the SQL. These functions are still a work-in-progress, so please let me know if you have problems.\ndbReadTable() is a simple wrapper around SELECT * FROM table. Use dbQuoteIdentifer() to safely quote the table name and prevent mismatches between the names allowed by R and the database.\ndbListTables() and dbExistsTable() let you determine what tables are available. If not provided by your database’s API, you may need to generate SQL that inspects the system tables.\ndbListFields() shows which fields are available in a given table.\ndbRemoveTable() wraps around DROP TABLE. Start with SQL::sqlTableDrop().\nImplement dbBegin(), dbCommit() and dbRollback() to provide basic transaction support. This functionality is currently not tested in the DBItest package."
  },
  {
    "href": "advanced/backend.html#metadata-methods",
    "title": "Implementing a new backend",
    "section": "Metadata methods",
    "text": "There are a lot of extra metadata methods for result sets (and one for the connection) that you might want to implement, as well. They are described in the following.\n\ndbIsValid() returns if a connection or a result set is open (TRUE) or closed (FALSE). All further methods in this section are valid for result sets only.\ndbGetStatement() returns the issued query as a character value.\ndbColumnInfo() lists the names and types of the result set’s columns.\ndbGetRowCount() and dbGetRowsAffected() returns the number of rows returned or altered in a SELECT or INSERT/UPDATE query, respectively.\ndbBind() allows using parametrised queries. Take a look at sqlInterpolate() and sqlParseVariables() if your SQL engine doesn’t offer native parametrised queries."
  },
  {
    "href": "advanced/backend.html#full-dbi-compliance",
    "title": "Implementing a new backend",
    "section": "Full DBI compliance",
    "text": "By now, your package should implement all methods defined in the DBI specification. If you want to walk the extra mile, offer a read-only mode that allows your users to be sure that their valuable data doesn’t get destroyed inadvertently."
  },
  {
    "href": "advanced/contract.html#connection-opened",
    "title": "Connections Contract",
    "section": "Connection Opened",
    "text": "When a new connection is opened, your R package should inform the Connections Pane by calling connectionOpened(). For example:\nobserver <- getOption(\"connectionObserver\")\nif (!is.null(observer))\n  observer$connectionOpened(...)\n\nArguments\nThe arguments to the connectionOpened() function are as follows:\n\n\n\n\n\n\n\nArgument\nValue\n\n\n\n\ntype\nFree-form text; the type of data connection (e.g. “SQL”).\n\n\ndisplayName\nFree-form text; the name shown to the user in the Connections Pane.\n\n\nhost\nThe name of the server/host being connected to; optional.\n\n\nicon\nThe full path to a small, square PNG icon representing the connection; optional.\n\n\nconnectCode\nA snippet of R code which can be used to open the connection again.\n\n\ndisconnect\nA function which can be used to close the connection.\n\n\nlistObjectTypes\nA function which returns the hierarchy of object types returned by the connection, as a nested list; see Specifying Objects below for details.\n\n\nlistObjects\nA function which lists top-level objects in the database when called without arguments, or the objects inside some other object when invoked with an object specifier. The return value is a data frame with name and type columns.\n\n\nlistColumns\nA function which lists the columns of a data object. The return value is a data frame with name and type columns.\n\n\npreviewObject\nA function accepting a row limit and an object specifier; it returns the given number of rows from the data object as a data frame.\n\n\nactions\nA named list of actions which can be performed on the connection. Each list entry should be a list with icon (path to an small, square PNG representing the action) and callback (function to perform when the action is invoked)\n\n\nconnectionObject\nThe raw connection object.\n\n\n\n\n\nSpecifying Objects\n\nHierarchy\nThe listObjectTypes() function is invoked by RStudio to discover the hierarchy of objects supported by the connection. The return value should be a nested list, where each entry has a contains member that indicates what the object type contains, and optionally an icon member indicating the path to a small, square PNG representing the object type.\nThe contains member is either a list of object types that the object contains, or the special value \"data\" if the object contains data.\nFor example, if your database has schemas, and schemas can have tables and views, you might return a list like the following:\nlist(\n  schema = list(\n    icon = \"path/to/schema.png\",\n    contains = list(\n      table = list(\n        contains = \"data\"),\n      view = list(\n        contains = \"data\"))))\n\n\nArguments\nThe listObjects(), listColumns(), and previewObject() functions are invoked by RStudio as the user explores the objects and data in the connection. They are all invoked with an object specifier, which is a set of named arguments corresponding to the object types returned by listObjectTypes().\nFor instance, suppose the user opens a connection, expands the schema “foo”, and then previews the table “bar” inside that schema. RStudio will invoke the functions as follows:\nlistObjects()                  # returns all schema\nlistObjects(schema = \"foo\")    # returns tables and views in \"foo\"\npreviewObject(schema = \"foo\",  # returns data in foo.bar\n              table  = \"bar\")\n\n\n\nPersistence\nWhen your R package informs RStudio that a connection has been opened via connectionOpened(), RStudio saves some of the connection’s metadata. Even after the connection is closed, RStudio shows users the connection along with the code (which is supplied in the connectCode argument to connectionOpened as described above) to re-open it. It’s therefore important to supply a connectCode string which will work with few prerequisites."
  },
  {
    "href": "advanced/contract.html#connection-updated",
    "title": "Connections Contract",
    "section": "Connection Updated",
    "text": "If the list of objects in your data source changes while the connection is open, your R package can tell RStudio to refresh the Connections Pane to show the new information. This is done by invoking the connectionUpdated() method as follows:\nobserver <- getOption(\"connectionObserver\")\nif (!is.null(observer))\n  observer$connectionUpdated(type, host)\nwhere type and host match the type and host parameters given when the connection was opened."
  },
  {
    "href": "advanced/contract.html#connection-closed",
    "title": "Connections Contract",
    "section": "Connection Closed",
    "text": "When the user closes the connection, your R package should tell RStudio to update the pane. This done by invoking the connectionClosed() method, which works identically to connectionUpdated():\nobserver <- getOption(\"connectionObserver\")\nif (!is.null(observer))\n  observer$connectionClosed(type, host)"
  },
  {
    "href": "advanced/contract.html#examples",
    "title": "Connections Contract",
    "section": "Examples",
    "text": "There are currently two packages which implement the connections contract. You may use these as examples:\n\nodbc\nsparklyr"
  },
  {
    "href": "LICENSE.html#creative-commons",
    "title": "Databases using R",
    "section": "creative commons",
    "text": ""
  },
  {
    "href": "LICENSE.html#creative-commons-attribution-sharealike-4.0-international-public-license",
    "title": "Databases using R",
    "section": "Creative Commons Attribution-ShareAlike 4.0 International Public License",
    "text": "By exercising the Licensed Rights (defined below), You accept and agree to be bound by the terms and conditions of this Creative Commons Attribution-ShareAlike 4.0 International Public License (“Public License”). To the extent this Public License may be interpreted as a contract, You are granted the Licensed Rights in consideration of Your acceptance of these terms and conditions, and the Licensor grants You such rights in consideration of benefits the Licensor receives from making the Licensed Material available under these terms and conditions.\n\nSection 1 – Definitions.\n\nAdapted Material means material subject to Copyright and Similar Rights that is derived from or based upon the Licensed Material and in which the Licensed Material is translated, altered, arranged, transformed, or otherwise modified in a manner requiring permission under the Copyright and Similar Rights held by the Licensor. For purposes of this Public License, where the Licensed Material is a musical work, performance, or sound recording, Adapted Material is always produced where the Licensed Material is synched in timed relation with a moving image.\nAdapter’s License means the license You apply to Your Copyright and Similar Rights in Your contributions to Adapted Material in accordance with the terms and conditions of this Public License.\nBY-SA Compatible License means a license listed at creativecommons.org/compatiblelicenses, approved by Creative Commons as essentially the equivalent of this Public License.\nCopyright and Similar Rights means copyright and/or similar rights closely related to copyright including, without limitation, performance, broadcast, sound recording, and Sui Generis Database Rights, without regard to how the rights are labeled or categorized. For purposes of this Public License, the rights specified in Section 2(b)(1)-(2) are not Copyright and Similar Rights.\nEffective Technological Measures means those measures that, in the absence of proper authority, may not be circumvented under laws fulfilling obligations under Article 11 of the WIPO Copyright Treaty adopted on December 20, 1996, and/or similar international agreements.\nExceptions and Limitations means fair use, fair dealing, and/or any other exception or limitation to Copyright and Similar Rights that applies to Your use of the Licensed Material.\nLicense Elements means the license attributes listed in the name of a Creative Commons Public License. The License Elements of this Public License are Attribution and ShareAlike.\nLicensed Material means the artistic or literary work, database, or other material to which the Licensor applied this Public License.\nLicensed Rights means the rights granted to You subject to the terms and conditions of this Public License, which are limited to all Copyright and Similar Rights that apply to Your use of the Licensed Material and that the Licensor has authority to license.\nLicensor means the individual(s) or entity(ies) granting rights under this Public License.\nShare means to provide material to the public by any means or process that requires permission under the Licensed Rights, such as reproduction, public display, public performance, distribution, dissemination, communication, or importation, and to make material available to the public including in ways that members of the public may access the material from a place and at a time individually chosen by them.\nSui Generis Database Rights means rights other than copyright resulting from Directive 96/9/EC of the European Parliament and of the Council of 11 March 1996 on the legal protection of databases, as amended and/or succeeded, as well as other essentially equivalent rights anywhere in the world.\nYou means the individual or entity exercising the Licensed Rights under this Public License. Your has a corresponding meaning.\n\n\n\nSection 2 – Scope.\n\nLicense grant.\n\nSubject to the terms and conditions of this Public License, the Licensor hereby grants You a worldwide, royalty-free, non-sublicensable, non-exclusive, irrevocable license to exercise the Licensed Rights in the Licensed Material to:\nA. reproduce and Share the Licensed Material, in whole or in part; and\nB. produce, reproduce, and Share Adapted Material.\nExceptions and Limitations. For the avoidance of doubt, where Exceptions and Limitations apply to Your use, this Public License does not apply, and You do not need to comply with its terms and conditions.\nTerm. The term of this Public License is specified in Section 6(a).\nMedia and formats; technical modifications allowed. The Licensor authorizes You to exercise the Licensed Rights in all media and formats whether now known or hereafter created, and to make technical modifications necessary to do so. The Licensor waives and/or agrees not to assert any right or authority to forbid You from making technical modifications necessary to exercise the Licensed Rights, including technical modifications necessary to circumvent Effective Technological Measures. For purposes of this Public License, simply making modifications authorized by this Section 2(a)(4) never produces Adapted Material.\nDownstream recipients.\nA. Offer from the Licensor – Licensed Material. Every recipient of the Licensed Material automatically receives an offer from the Licensor to exercise the Licensed Rights under the terms and conditions of this Public License.\nB. __Additional offer from the Licensor – Adapted Material. Every recipient of Adapted Material from You automatically receives an offer from the Licensor to exercise the Licensed Rights in the Adapted Material under the conditions of the Adapter’s License You apply.\nC. No downstream restrictions. You may not offer or impose any additional or different terms or conditions on, or apply any Effective Technological Measures to, the Licensed Material if doing so restricts exercise of the Licensed Rights by any recipient of the Licensed Material.\nNo endorsement. Nothing in this Public License constitutes or may be construed as permission to assert or imply that You are, or that Your use of the Licensed Material is, connected with, or sponsored, endorsed, or granted official status by, the Licensor or others designated to receive attribution as provided in Section 3(a)(1)(A)(i).\n\nOther rights.\n\nMoral rights, such as the right of integrity, are not licensed under this Public License, nor are publicity, privacy, and/or other similar personality rights; however, to the extent possible, the Licensor waives and/or agrees not to assert any such rights held by the Licensor to the limited extent necessary to allow You to exercise the Licensed Rights, but not otherwise.\nPatent and trademark rights are not licensed under this Public License.\nTo the extent possible, the Licensor waives any right to collect royalties from You for the exercise of the Licensed Rights, whether directly or through a collecting society under any voluntary or waivable statutory or compulsory licensing scheme. In all other cases the Licensor expressly reserves any right to collect such royalties.\n\n\n\n\nSection 3 – License Conditions.\nYour exercise of the Licensed Rights is expressly made subject to the following conditions.\n\nAttribution.\n\nIf You Share the Licensed Material (including in modified form), You must:\nA. retain the following if it is supplied by the Licensor with the Licensed Material:\n\nidentification of the creator(s) of the Licensed Material and any others designated to receive attribution, in any reasonable manner requested by the Licensor (including by pseudonym if designated);\na copyright notice;\na notice that refers to this Public License;\na notice that refers to the disclaimer of warranties;\na URI or hyperlink to the Licensed Material to the extent reasonably practicable;\n\nB. indicate if You modified the Licensed Material and retain an indication of any previous modifications; and\nC. indicate the Licensed Material is licensed under this Public License, and include the text of, or the URI or hyperlink to, this Public License.\nYou may satisfy the conditions in Section 3(a)(1) in any reasonable manner based on the medium, means, and context in which You Share the Licensed Material. For example, it may be reasonable to satisfy the conditions by providing a URI or hyperlink to a resource that includes the required information.\nIf requested by the Licensor, You must remove any of the information required by Section 3(a)(1)(A) to the extent reasonably practicable.\n\nShareAlike.\n\nIn addition to the conditions in Section 3(a), if You Share Adapted Material You produce, the following conditions also apply.\n\nThe Adapter’s License You apply must be a Creative Commons license with the same License Elements, this version or later, or a BY-SA Compatible License.\nYou must include the text of, or the URI or hyperlink to, the Adapter’s License You apply. You may satisfy this condition in any reasonable manner based on the medium, means, and context in which You Share Adapted Material.\nYou may not offer or impose any additional or different terms or conditions on, or apply any Effective Technological Measures to, Adapted Material that restrict exercise of the rights granted under the Adapter’s License You apply.\n\n\n\nSection 4 – Sui Generis Database Rights.\nWhere the Licensed Rights include Sui Generis Database Rights that apply to Your use of the Licensed Material:\n\nfor the avoidance of doubt, Section 2(a)(1) grants You the right to extract, reuse, reproduce, and Share all or a substantial portion of the contents of the database;\nif You include all or a substantial portion of the database contents in a database in which You have Sui Generis Database Rights, then the database in which You have Sui Generis Database Rights (but not its individual contents) is Adapted Material, including for purposes of Section 3(b); and\nYou must comply with the conditions in Section 3(a) if You Share all or a substantial portion of the contents of the database.\n\nFor the avoidance of doubt, this Section 4 supplements and does not replace Your obligations under this Public License where the Licensed Rights include other Copyright and Similar Rights.\n\n\nSection 5 – Disclaimer of Warranties and Limitation of Liability.\n\nUnless otherwise separately undertaken by the Licensor, to the extent possible, the Licensor offers the Licensed Material as-is and as-available, and makes no representations or warranties of any kind concerning the Licensed Material, whether express, implied, statutory, or other. This includes, without limitation, warranties of title, merchantability, fitness for a particular purpose, non-infringement, absence of latent or other defects, accuracy, or the presence or absence of errors, whether or not known or discoverable. Where disclaimers of warranties are not allowed in full or in part, this disclaimer may not apply to You.\nTo the extent possible, in no event will the Licensor be liable to You on any legal theory (including, without limitation, negligence) or otherwise for any direct, special, indirect, incidental, consequential, punitive, exemplary, or other losses, costs, expenses, or damages arising out of this Public License or use of the Licensed Material, even if the Licensor has been advised of the possibility of such losses, costs, expenses, or damages. Where a limitation of liability is not allowed in full or in part, this limitation may not apply to You.\nThe disclaimer of warranties and limitation of liability provided above shall be interpreted in a manner that, to the extent possible, most closely approximates an absolute disclaimer and waiver of all liability.\n\n\n\nSection 6 – Term and Termination.\n\nThis Public License applies for the term of the Copyright and Similar Rights licensed here. However, if You fail to comply with this Public License, then Your rights under this Public License terminate automatically.\nWhere Your right to use the Licensed Material has terminated under Section 6(a), it reinstates:\n\nautomatically as of the date the violation is cured, provided it is cured within 30 days of Your discovery of the violation; or\nupon express reinstatement by the Licensor.\n\nFor the avoidance of doubt, this Section 6(b) does not affect any right the Licensor may have to seek remedies for Your violations of this Public License.\nFor the avoidance of doubt, the Licensor may also offer the Licensed Material under separate terms or conditions or stop distributing the Licensed Material at any time; however, doing so will not terminate this Public License.\nSections 1, 5, 6, 7, and 8 survive termination of this Public License.\n\n\n\nSection 7 – Other Terms and Conditions.\n\nThe Licensor shall not be bound by any additional or different terms or conditions communicated by You unless expressly agreed.\nAny arrangements, understandings, or agreements regarding the Licensed Material not stated herein are separate from and independent of the terms and conditions of this Public License.t stated herein are separate from and independent of the terms and conditions of this Public License.\n\n\n\nSection 8 – Interpretation.\n\nFor the avoidance of doubt, this Public License does not, and shall not be interpreted to, reduce, limit, restrict, or impose conditions on any use of the Licensed Material that could lawfully be made without permission under this Public License.\nTo the extent possible, if any provision of this Public License is deemed unenforceable, it shall be automatically reformed to the minimum extent necessary to make it enforceable. If the provision cannot be reformed, it shall be severed from this Public License without affecting the enforceability of the remaining terms and conditions.\nNo term or condition of this Public License will be waived and no failure to comply consented to unless expressly agreed to by the Licensor.\nNothing in this Public License constitutes or may be interpreted as a limitation upon, or waiver of, any privileges and immunities that apply to the Licensor or You, including from the legal processes of any jurisdiction or authority.\n\n\nCreative Commons is not a party to its public licenses. Notwithstanding, Creative Commons may elect to apply one of its public licenses to material it publishes and in those instances will be considered the “Licensor.” Except for the limited purpose of indicating that material is shared under a Creative Commons public license or as otherwise permitted by the Creative Commons policies published at creativecommons.org/policies, Creative Commons does not authorize the use of the trademark “Creative Commons” or any other trademark or logo of Creative Commons without its prior written consent including, without limitation, in connection with any unauthorized modifications to any of its public licenses or any other arrangements, understandings, or agreements concerning use of licensed material. For the avoidance of doubt, this paragraph does not form part of the public licenses.\nCreative Commons may be contacted at creativecommons.org"
  },
  {
    "href": "tooling/pro-drivers.html#coverage",
    "title": "RStudio Professional Drivers",
    "section": "Coverage",
    "text": "We connect you to some of the most popular databases available today, and we are committed to increasing the number of data connectors we support in the future.\n\n\n\n\nMicrosoft SQL Server\nOracle\nTeradata\nPostgreSQL\nApache Hive\nApache Impala\nApache Cassandra\nAmazon Athena\nAmazon Redshift\nMongoDB\nGoogle BigQuery\nIBM Netezza\nSalesforce\nMySQL\nSnowflake\n\n\n\nFor more information and to download the drivers →"
  },
  {
    "href": "tooling/connections.html#connection-history-and-existing-connections",
    "title": "Databases using R",
    "section": "Connection history and existing connections",
    "text": "The Connections Pane shows you all the connections you have made to supported data sources, and lets you know which connections are currently active. RStudio supports multiple, simultaneous database connections. This history is specific to your user account and is stored privately in RStudio’s settings; you will see it in all your RStudio projects."
  },
  {
    "href": "tooling/connections.html#connecting-to-a-database-in-r",
    "title": "Databases using R",
    "section": "Connecting to a Database in R",
    "text": "In RStudio, there are two ways to connect to a database:\n\nWrite the connection code manually.  See this article for more information: Connecting to a Database in R\nUse the New Connection interface. The dialog lists all the connection types and drivers it can find on the system. See the next section for more information."
  },
  {
    "href": "tooling/connections.html#new-connection-interface",
    "title": "Databases using R",
    "section": "New Connection interface",
    "text": "The New Connection interface will list the following:\n\n\n\n\nODBC DSNs installed on your system If the odbc package is installed, any system DSN (Data Source Name) you have installed will be listed. For more information, see Integrated security with DSN.\nConnections supplied by your administrator An administrator may put connection snippets into a folder to make them available to end users. If you are an administrator, see our page on RStudio Connection Snippet Files for more information.\nODBC drivers installed on your system If the odbc package is installed, any ODBC driver you have installed on your system will be listed. The method for installing ODBC drivers varies by system. See our page on Setting up ODBC Drivers. Because the resulting connection code in the New Connection interface will have to be changed significantly, it may be better to write the connection code manually. See this article for more information: Connecting to a Database in R"
  },
  {
    "href": "tooling/connections.html#connections-pane",
    "title": "Databases using R",
    "section": "Connections pane",
    "text": "After successfully connecting to a database using odbc, the Connections pane will do the following:\n\nEnable navigation by displaying an expandable list with the hierarchy of databases, schema, tables and fields\n\nAllow you to preview the top 1,000 rows of a table by clicking in the icon to the right of the table’s name\nClose the database connection by simply clicking on the corresponding button inside the pane"
  },
  {
    "href": "databases/salesforce.html#driver-options",
    "title": "Salesforce",
    "section": "Driver Options",
    "text": "RStudio Professional Drivers - RStudio Workbench (formerly RStudio Server Pro), RStudio Desktop Pro, RStudio Connect, or Shiny Server Pro users can download and use RStudio Professional Drivers at no additional charge. These drivers include an ODBC connector for Salesforce. RStudio delivers standards-based, supported, professional ODBC drivers. Use RStudio Professional Drivers when you run R or Shiny with your production systems. See the RStudio Professional Drivers for more information."
  },
  {
    "href": "databases/salesforce.html#package-options",
    "title": "Salesforce",
    "section": "Package Options",
    "text": "The odbc package, in combination with a driver, provides DBI support and an ODBC connection."
  },
  {
    "href": "databases/salesforce.html#connection-settings",
    "title": "Salesforce",
    "section": "Connection Settings",
    "text": "There are four settings needed to make a connection:\n\nDriver - See the Drivers section for setup information\nUID - The user’s network ID or server local account\nPWD - The account’s password\nSecurityToken - The account’s password\n\ncon <- DBI::dbConnect(odbc::odbc(),\n                    Driver        = \"[your driver's name]\",\n                    UID           = rstudioapi::askForPassword(\"Database user\"),\n                    PWD           = rstudioapi::askForPassword(\"Database password\"),\n                    SecurityToken = rstudioapi::askForPassword(\"Security token\"))\nA preferred method is to use the config package cache the credentials:\nusername <- config::get(\"sf_user\")\npassword <- config::get(\"sf_password\")\ntoken    <- config::get(\"sf_token\")\n\ncon <- DBI::dbConnect(odbc::odbc(), \n                      Driver=\"Salesforce\", \n                      UID = username, \n                      PWD = password, \n                      SecurityToken = token)\nThe config.yml file for the connection above would be something like this:\ndefault:\n  sf_user: \"[User name]\"\n  sf_password: \"[Password]\"\n  sf_token: \"[User's token]\""
  },
  {
    "href": "databases/salesforce.html#dplyr-translation",
    "title": "Salesforce",
    "section": "dplyr translation",
    "text": "dplyr currently does not support the translation of Salesforce queries."
  },
  {
    "href": "databases/snowflake.html#driver-options",
    "title": "Snowflake",
    "section": "Driver Options",
    "text": "Snowflake - Download the ODBC driver directly from Snowflake’s site: Configure an ODBC Connection\nRStudio Professional Drivers - RStudio Workbench (formerly RStudio Server Pro), RStudio Desktop Pro, RStudio Connect, or Shiny Server Pro users can download and use RStudio Professional Drivers at no additional charge. These drivers include an ODBC connector for Redshift databases. RStudio delivers standards-based, supported, professional ODBC drivers. Use RStudio Professional Drivers when you run R or Shiny with your production systems. See the RStudio Professional Drivers for more information."
  },
  {
    "href": "databases/snowflake.html#package-options",
    "title": "Snowflake",
    "section": "Package Options",
    "text": "The odbc package, in combination with a driver, provides DBI support and an ODBC connection."
  },
  {
    "href": "databases/snowflake.html#connection-settings",
    "title": "Snowflake",
    "section": "Connection Settings",
    "text": "There are seven settings needed to make a connection:\n\nDriver - See the Drivers section for setup information\nServer - The URL to the database server\nUID - The user’s account ID\nPWD - The account’s password\nDatabase - The database name within the server\nWarehouse - The warehouse name within the database\nSchema - The schema name within the warehouse\n\n\ncon <- DBI::dbConnect(odbc::odbc(),\n                      Driver       = \"[your driver's name]\",\n                      Server       = \"[your server's path]\",\n                      UID          = rstudioapi::askForPassword(\"Database user\"),\n                      PWD          = rstudioapi::askForPassword(\"Database password\"),\n                      Database     = \"[your database's name]\",\n                      Warehouse    = \"[your warehouse's name]\",\n                      Schema       = \"[your schema's name]\"\n                      )"
  },
  {
    "href": "databases/my-sql.html#using-the-odbc-package",
    "title": "MySQL",
    "section": "Using the odbc package",
    "text": "The odbc package, in combination with a driver, provides DBI support and an ODBC connection.\n\nDriver options\n\nMySQL - The official MySQL website provides a downloadable ODBC driver for MySQL: MySQL Connector\nMariaDB - MariaDB is an open source relational database built by the original developers of MySQL. MariaDB provides an ODBC connector that can be used as a drop-in replacement for a MySQL ODBC connector: MariaDB Connector\nRStudio Professional Drivers - RStudio Workbench (formerly RStudio Server Pro), RStudio Desktop Pro, RStudio Connect, or Shiny Server Pro users can download and use RStudio Professional Drivers at no additional charge. These drivers include an ODBC connector for MySQL databases. RStudio delivers standards-based, supported, professional ODBC drivers. Use RStudio Professional Drivers when you run R or Shiny with your production systems. See the RStudio Professional Drivers for more information.\n\n\n\nConnection Settings\nThere are 5 settings needed to make a connection:\n\nDriver - See the Drivers section for setup information\nServer - A network path to the database server\nUID - User name used to access MySQL server\nPWD - The password corresponding to the provided UID\nPort - Should be set to 3306\n\n\ncon <- DBI::dbConnect(odbc::odbc(),\n                      Driver   = \"[your driver's name]\",\n                      Server   = \"[your server's path]\",\n                      UID      = rstudioapi::askForPassword(\"Database user\"),\n                      PWD      = rstudioapi::askForPassword(\"Database password\"),\n                      Port     = 3306)"
  },
  {
    "href": "databases/my-sql.html#using-the-rmariadb-package",
    "title": "MySQL",
    "section": "Using the RMariaDB package",
    "text": "RMariaDB is a database interface and MariaDB driver for R. This version is aimed at full compliance with the DBI specification, as a replacement for the old RMySQL package. For more information, visit RMariaDB’s official site: rmariadb.r-dbi.org\nTo install from CRAN:\ninstall.packages(\"RMariaDB\")\nThe development version from github:\nTo install the development version:\n# install.packages(\"remotes\")\nremotes::install_github(\"r-dbi/DBI\")\nremotes::install_github(\"r-dbi/RMariaDB\")\nTo connect:\nlibrary(DBI)\n# Connect to my-db as defined in ~/.my.cnf\ncon <- dbConnect(RMariaDB::MariaDB(), group = \"my-db\")"
  },
  {
    "href": "databases/redshift.html#driver-options",
    "title": "Amazon Redshift",
    "section": "Driver Options",
    "text": "Amazon - The Amazon AWS website provides instructions on how to download and setup their driver: Configure an ODBC Connection\nRStudio Professional Drivers - RStudio Workbench (formerly RStudio Server Pro), RStudio Desktop Pro, RStudio Connect, or Shiny Server Pro users can download and use RStudio Professional Drivers at no additional charge. These drivers include an ODBC connector for Redshift databases. RStudio delivers standards-based, supported, professional ODBC drivers. Use RStudio Professional Drivers when you run R or Shiny with your production systems. See the RStudio Professional Drivers for more information."
  },
  {
    "href": "databases/redshift.html#package-options",
    "title": "Amazon Redshift",
    "section": "Package Options",
    "text": "The odbc package, in combination with a driver, provides DBI support and an ODBC connection."
  },
  {
    "href": "databases/redshift.html#connection-settings",
    "title": "Amazon Redshift",
    "section": "Connection Settings",
    "text": "There are six settings needed to make a connection:\n\nDriver - See the Drivers section for setup information\nservername - A network path to the database server\ndatabase - The name of the schema\nUID - The user’s network ID or server local account\nPWD - The account’s password\nPort - Should be set to 5439\n\n\ncon <- DBI::dbConnect(odbc::odbc(),\n                      Driver       = \"[your driver's name]\",\n                      servername   = \"[your server's path]\",\n                      database     = \"[your database's name]\",\n                      UID          = rstudioapi::askForPassword(\"Database user\"),\n                      PWD          = rstudioapi::askForPassword(\"Database password\"),\n                      Port         = 5439)"
  },
  {
    "href": "databases/athena.html#driver-options",
    "title": "Athena",
    "section": "Driver Options",
    "text": "Amazon Athena - Please refer to the Athena’s website for instructions on how to download and setup their official driver: Athena ODBC page\nRStudio Professional Drivers - RStudio Workbench (formerly RStudio Server Pro), RStudio Desktop Pro, RStudio Connect, or Shiny Server Pro users can download and use RStudio Professional Drivers at no additional charge. These drivers include an ODBC connector for Apache Impala. RStudio delivers standards-based, supported, professional ODBC drivers. Use RStudio Professional Drivers when you run R or Shiny with your production systems. See the RStudio Professional Drivers for more information."
  },
  {
    "href": "databases/athena.html#package-options",
    "title": "Athena",
    "section": "Package Options",
    "text": "The odbc package, in combination with a driver, provides DBI support and an ODBC connection."
  },
  {
    "href": "databases/athena.html#connection-settings",
    "title": "Athena",
    "section": "Connection Settings",
    "text": "There are settings needed to make a connection:\n\nDriver - See the Drivers section for setup information\nS3OutputLocation - S3 bucket for Athena output\nAwsRegion - AWS region where the service is available\nAuthenticationType - Use IAM Credentials if using the AWS Access Key and Secret as for credentials\nSchema - Default Schema name to use when interacting with the database\nUID - The AWS Access Key that will be used to connect\nPWD - The AWS Secret that will be used to connect\n\n\ncon <- DBI::dbConnect(\n  odbc::odbc(),\n  Driver             = \"[your driver's name]\",\n  S3OutputLocation   = \"[your S3 bucket]\",\n  AwsRegion          = \"[your AWS region bucket]\",\n  AuthenticationType = \"IAM Credentials\",\n  Schema             = \"[your schema's name]\",\n  UID                = rstudioapi::askForPassword(\"AWS Access Key\"),\n  PWD                = rstudioapi::askForPassword(\"AWS Secret Key\")\n  )"
  },
  {
    "href": "databases/netezza.html#driver-options",
    "title": "Netezza",
    "section": "Driver Options",
    "text": "RStudio Professional Drivers - RStudio Workbench (formerly RStudio Server Pro), RStudio Desktop Pro, RStudio Connect, or Shiny Server Pro users can download and use RStudio Professional Drivers at no additional charge. These drivers include an ODBC connector for Apache Hive. RStudio delivers standards-based, supported, professional ODBC drivers. Use RStudio Professional Drivers when you run R or Shiny with your production systems. See the RStudio Professional Drivers for more information."
  },
  {
    "href": "databases/netezza.html#package-options",
    "title": "Netezza",
    "section": "Package Options",
    "text": "The odbc package, in combination with a driver, provides DBI support and an ODBC connection."
  },
  {
    "href": "databases/netezza.html#connection-settings",
    "title": "Netezza",
    "section": "Connection Settings",
    "text": "There are six settings needed to make a connection:\n\nDriver - See the Drivers section for setup information\nServer - The database server’s IP address\nPort - Should be set to 5480\nDatabase - The name of the database to default to\nUID - The user’s network ID or server local account\nPWD - The account’s password\n\n\ncon <- DBI::dbConnect(odbc::odbc(),\n                      Driver = \"[your driver's name]\",\n                      Server   = \"[your server's path]\",\n                      Port = \"5480\",\n                      Database = \"[your database's name]\",\n                      UID    = rstudioapi::askForPassword(\"Database user\"),\n                      PWD    = rstudioapi::askForPassword(\"Database password\")\n                      )"
  },
  {
    "href": "databases/oracle.html#driver-options",
    "title": "Oracle",
    "section": "Driver Options",
    "text": "Connections to Oracle require the Oracle Instant Client to be installed.\n\nOracle - Please refer to Oracle’s website for instructions on how to download and setup their official driver: Oracle ODBC driver page\nRStudio Professional Drivers - RStudio Workbench (formerly RStudio Server Pro), RStudio Desktop Pro, RStudio Connect, or Shiny Server Pro users can download and use RStudio Professional Drivers at no additional charge. These drivers include an ODBC connector for Oracle databases. RStudio delivers standards-based, supported, professional ODBC drivers. Use RStudio Professional Drivers when you run R or Shiny with your production systems. See the RStudio Professional Drivers for more information."
  },
  {
    "href": "databases/oracle.html#package-options",
    "title": "Oracle",
    "section": "Package Options",
    "text": "There are packages that either connect via ODBC but do not provide support for DBI, or offer DBI support but connect via JDBC. The odbc package, in combination with a driver, satisfies both requirements.\nAnother package that provides both ODBC connectivity and DBI support is ROracle. The current version of dbplyr in CRAN does not yet fully support a connection coming from ROracle, but we are working on it."
  },
  {
    "href": "databases/oracle.html#connection-settings",
    "title": "Oracle",
    "section": "Connection Settings",
    "text": "There are six settings needed to make a connection:\n\nDriver - See the Drivers section for more setup information\nHost - A network path to the database server\nSVC - The name of the schema\nUID - The user’s network ID or server local account\nPWD - The account’s password\nPort - Should be set to 1521\n\n\ncon <- DBI::dbConnect(odbc::odbc(),\n                      Driver = \"[your driver's name]\",\n                      Host   = \"[your server's path]\",\n                      SVC    = \"[your schema's name]\",\n                      UID    = rstudioapi::askForPassword(\"Database user\"),\n                      PWD    = rstudioapi::askForPassword(\"Database password\"),\n                      Port   = 1521)"
  },
  {
    "href": "databases/oracle.html#dplyr-translation",
    "title": "Oracle",
    "section": "dplyr Translation",
    "text": "Known Issues\nThis section will be updated as new issues are found, and when fixes are applied in the development version of the dbplyr package. The issue will be fully removed when the fix is part of of the package’s version in CRAN.\n\nRJDBC support - Even though it is not considered an issue, we have found a workaround. The approach is to point the current JDBC connection to the Oracle translation inside dbplyr:\n\nsql_translation.JDBCConnection <- dbplyr:::sql_translation.Oracle\nsql_select.JDBCConnection <- dbplyr:::sql_query_select.Oracle\nsql_subquery.JDBCConnection <- dbplyr:::sql_query_wrap.Oracle\nPlease refer to the Issues section in dplyr to find out the latest regarding bugs and resolutions."
  },
  {
    "href": "databases/teradata.html#driver-options",
    "title": "Teradata",
    "section": "Driver Options",
    "text": "Teradata Downloads - Download and install the driver made available by Teradata in their Connectivity page\nRStudio Professional Drivers - RStudio Workbench (formerly RStudio Server Pro), RStudio Desktop Pro, RStudio Connect, or Shiny Server Pro users can download and use RStudio Professional Drivers at no additional charge. These drivers include an ODBC connector for Apache Impala. RStudio delivers standards-based, supported, professional ODBC drivers. Use RStudio Professional Drivers when you run R or Shiny with your production systems. See the RStudio Professional Drivers for more information."
  },
  {
    "href": "databases/teradata.html#package-options",
    "title": "Teradata",
    "section": "Package Options",
    "text": "The odbc package, in combination with a driver, provides DBI support and an ODBC connection."
  },
  {
    "href": "databases/teradata.html#connection-settings",
    "title": "Teradata",
    "section": "Connection Settings",
    "text": "There are settings needed to make a connection:\n\nDriver - See the Drivers section for setup information\nHost - A network path to the database server\nDBCName - IP address or Alias of the server\nUID - The user’s network ID or server local account\nPWD - The account’s password\n\n\ncon <- DBI::dbConnect(odbc::odbc(),\n                      Driver = \"[your driver's name]\",\n                      Host   = \"[your server's path]\",\n                      DBCName = \"[IP Address or Alias]\"\n                      UID    = rstudioapi::askForPassword(\"Database user\"),\n                      PWD    = rstudioapi::askForPassword(\"Database password\"))"
  },
  {
    "href": "databases/microsoft-sql-server.html#driver-options",
    "title": "Microsoft SQL Server",
    "section": "Driver Options",
    "text": "Microsoft Windows - The ODBC database drivers are usually pre-installed with the Windows operating systems.\nLinux and Apple MacOS - This is the link to the Microsoft Docs site that outlines how to install the driver based on your specific Operating System: Installing the Microsoft ODBC Driver for SQL Server on Linux and macOS\nRStudio Professional Drivers - RStudio Workbench (formerly RStudio Server Pro), RStudio Desktop Pro, RStudio Connect, or Shiny Server Pro users can download and use RStudio Professional Drivers at no additional charge. These drivers include an ODBC connector for Microsoft SQL Server databases. RStudio delivers standards-based, supported, professional ODBC drivers. Use RStudio Professional Drivers when you run R or Shiny with your production systems. See the RStudio Professional Drivers for more information."
  },
  {
    "href": "databases/microsoft-sql-server.html#package-options",
    "title": "Microsoft SQL Server",
    "section": "Package Options",
    "text": "There are several that either connect via ODBC but do not provide support for DBI, or offer DBI support but connect via JDBC. The odbc package, in combination with a driver, satisfies both requirements."
  },
  {
    "href": "databases/microsoft-sql-server.html#connection-settings",
    "title": "Microsoft SQL Server",
    "section": "Connection Settings",
    "text": "There are six settings needed to make a connection:\n\nDriver - See the Drivers section for more information\nServer - A network path to the database server\nDatabase - The name of the database\nUID - The user’s network ID or server local account\nPWD - The account’s password\nPort - Should be set to 1433\n\ncon <- DBI::dbConnect(odbc::odbc(),\n                      Driver   = \"[your driver's name]\",\n                      Server   = \"[your server's path]\",\n                      Database = \"[your database's name]\",\n                      UID      = rstudioapi::askForPassword(\"Database user\"),\n                      PWD      = rstudioapi::askForPassword(\"Database password\"),\n                      Port     = 1433)\n\nMS SQL Express\nThe following code shows how to connect to a local MS SQL Express instance:\ncon <- DBI::dbConnect(odbc::odbc(), \n                      Driver = \"SQL Server\", \n                      Server = \"localhost\\\\SQLEXPRESS\", \n                      Database = \"master\", \n                      Trusted_Connection = \"True\")"
  },
  {
    "href": "databases/microsoft-sql-server.html#dplyr-translation",
    "title": "Microsoft SQL Server",
    "section": "dplyr translation",
    "text": "Known Issues\nThis section will be updated as new issues are found, and when fixes are applied in the development version of the dbplyr package. The issue will be fully removed when the fix is part of of the package’s version in CRAN.\n\nRJDBC support - Even though it is not considered an issue, we have found a workaround. The approach is to point the current JDBC connection to the MS SQL translation inside dbplyr:\n\n  sql_translate_env.JDBCConnection <- dbplyr:::`sql_translate_env.Microsoft SQL Server`\n  sql_select.JDBCConnection <- dbplyr:::`sql_select.Microsoft SQL Server`\nPlease refer to the Issues section in dplyr to find out the latest regarding bugs and resolutions."
  },
  {
    "href": "databases/hive.html#driver-options",
    "title": "Apache Hive",
    "section": "Driver Options",
    "text": "Hadoop vendor - Download and install the driver made available by the Hadoop cluster provider (Cloudera, Hortonworks, etc.). To locate the driver please consult the vendor’s website.\nRStudio Professional Drivers - RStudio Workbench (formerly RStudio Server Pro), RStudio Desktop Pro, RStudio Connect, or Shiny Server Pro users can download and use RStudio Professional Drivers at no additional charge. These drivers include an ODBC connector for Apache Hive. RStudio delivers standards-based, supported, professional ODBC drivers. Use RStudio Professional Drivers when you run R or Shiny with your production systems. See the RStudio Professional Drivers for more information."
  },
  {
    "href": "databases/hive.html#package-options",
    "title": "Apache Hive",
    "section": "Package Options",
    "text": "The odbc package, in combination with a driver, provides DBI support and an ODBC connection."
  },
  {
    "href": "databases/hive.html#connection-settings",
    "title": "Apache Hive",
    "section": "Connection Settings",
    "text": "There are six settings needed to make a connection:\n\nDriver - See the Drivers section for setup information\nHost - A network path to the database server\nSchema - The name of the schema\nUID - The user’s network ID or server local account\nPWD - The account’s password\nPort - Should be set to 10000\n\n\ncon <- DBI::dbConnect(odbc::odbc(),\n                      Driver = \"[your driver's name]\",\n                      Host   = \"[your server's path]\",\n                      Schema = \"[your schema's name]\",\n                      UID    = rstudioapi::askForPassword(\"Database user\"),\n                      PWD    = rstudioapi::askForPassword(\"Database password\"),\n                      Port   = 10000)\n\n\nKnown Issues\n\nIt may be necessary to prevent Hive from returning the table prefix in column names. This can be done by setting hive.resultset.use.unique.column.names to false when establishing a connection to Hive. See tidyverse/dbplyr#573 for more context.\n\n\ncon <- DBI::dbConnect(odbc::odbc().\n                      ...,\n                      SSP_hive.resultset.use.unique.column.names = FALSE)"
  },
  {
    "href": "databases/cassandra.html#driver-options",
    "title": "Cassandra",
    "section": "Driver Options",
    "text": "RStudio Professional Drivers - RStudio Workbench (formerly RStudio Server Pro), RStudio Desktop Pro, RStudio Connect, or Shiny Server Pro users can download and use RStudio Professional Drivers at no additional charge. These drivers include an ODBC connector for Apache Impala. RStudio delivers standards-based, supported, professional ODBC drivers. Use RStudio Professional Drivers when you run R or Shiny with your production systems. See the RStudio Professional Drivers for more information."
  },
  {
    "href": "databases/cassandra.html#package-options",
    "title": "Cassandra",
    "section": "Package Options",
    "text": "The odbc package, in combination with a driver, provides DBI support and an ODBC connection."
  },
  {
    "href": "databases/cassandra.html#connection-settings",
    "title": "Cassandra",
    "section": "Connection Settings",
    "text": "There are settings needed to make a connection:\n\nDriver - See the Drivers section for setup information\nHost - A network path to the database server\nPort - Should be set to 9042\nKeyspace - Data replication namespace (not required)\nAuthMec - Use 0 for no authentication, and 1 for user name/password\nUID - The user’s network ID or server local account\nPWD - The account’s password\n\n\ncon <- DBI::dbConnect(odbc::odbc(),\n                      Driver   = \"[your driver's name]\",\n                      Host     = \"[your server's path or name]\",\n                      Port     = 9042,\n                      Keyspace = \"[your keypace's name]\",\n                      AuthMech = 1,\n                      UID      = rstudioapi::askForPassword(\"Database user\"),\n                      PWD      = rstudioapi::askForPassword(\"Database password\")\n                      )"
  },
  {
    "href": "databases/other-databases.html#odbc-dbi",
    "title": "Other Databases",
    "section": "odbc & DBI",
    "text": "Using an ODBC driver, you can still take advantage of the odbc and DBI packages to connect and query the database."
  },
  {
    "href": "databases/other-databases.html#best-practices",
    "title": "Other Databases",
    "section": "Best Practices",
    "text": "The Best Practices section offers advice that, in most cases, is independent from being on a dplyr-integrated database."
  },
  {
    "href": "databases/other-databases.html#contribute",
    "title": "Other Databases",
    "section": "Contribute",
    "text": "If you would like to take it a step further, consider contributing a translation to the dbplyr package. For more information, please see the SQL Translation page in this website."
  },
  {
    "href": "databases/postgresql.html#using-the-odbc-package",
    "title": "PostgreSQL",
    "section": "Using the odbc package",
    "text": "The odbc package, in combination with a driver, provides DBI support and an ODBC connection.\n\nDriver options\n\nPostgreSQL - The official PostgreSQL website provides instructions on how to download and setup their driver: psqlODBC - PostgreSQL ODBC driver\nRStudio Professional Drivers - RStudio Workbench (formerly RStudio Server Pro), RStudio Desktop Pro, RStudio Connect, or Shiny Server Pro users can download and use RStudio Professional Drivers at no additional charge. These drivers include an ODBC connector for PostgreSQL databases. RStudio delivers standards-based, supported, professional ODBC drivers. Use RStudio Professional Drivers when you run R or Shiny with your production systems. See the RStudio Professional Drivers for more information.\n\n\n\nConnection Settings\nThere are six settings needed to make a connection:\n\nDriver - See the Drivers section for setup information\nServer - A network path to the database server\nDatabase - The name of the schema\nUID - The user’s network ID or server local account\nPWD - The account’s password\nPort - Should be set to 5432\n\n\ncon <- DBI::dbConnect(odbc::odbc(),\n                      Driver   = \"[your driver's name]\",\n                      Server   = \"[your server's path]\",\n                      Database = \"[your database's name]\",\n                      UID      = rstudioapi::askForPassword(\"Database user\"),\n                      PWD      = rstudioapi::askForPassword(\"Database password\"),\n                      Port     = 5432)"
  },
  {
    "href": "databases/postgresql.html#using-the-rpostgres-package",
    "title": "PostgreSQL",
    "section": "Using the RPostgres package",
    "text": "RPostgres is an DBI-compliant interface to the postgres database. It is a ground-up rewrite using C++ and Rcpp. This package acts as both the database driver and the DBI interface. The code, and additional information are available in its GitHub repository here: RPostgres. To connect use:\nlibrary(DBI)\n# Connect to the default postgres database\ncon <- dbConnect(RPostgres::Postgres())\n\nKnown Issues\nParameterized queries - Using ? to mark parameters in a query does not currently work with RPostgres, instead use $. For more information, see the following issue: RPostgres/Issue/201\nairport <- dbSendQuery(con, \"SELECT * FROM airports WHERE faa = $1 or faa = $2\")\ndbBind(airport, list(\"GPT\", \"MSY\"))\ndbFetch(airport)"
  },
  {
    "href": "databases/big-query.html#using-the-odbc-package",
    "title": "Google BigQuery",
    "section": "Using the odbc package",
    "text": "The odbc package, in combination with a driver, provides DBI support and an ODBC connection.\n\nDriver options\n\nBigQuery - The official BigQuery website provides instructions on how to download and setup their ODBC driver: BigQuery Drivers\nRStudio Professional Drivers - RStudio Workbench (formerly RStudio Server Pro), RStudio Desktop Pro, RStudio Connect, or Shiny Server Pro users can download and use RStudio Professional Drivers at no additional charge. These drivers include an ODBC connector for Google BigQuery. RStudio delivers standards-based, supported, professional ODBC drivers. Use RStudio Professional Drivers when you run R or Shiny with your production systems. See the RStudio Professional Drivers for more information.\n\n\n\nConnection Settings\nThe easiest way to connect to BigQuery is through a Google service account. There are 5 settings needed to make a connection using a Google service account:\n\nDriver - See the Drivers section for setup information\nCatalog - The name of the BigQuery project\nEmail - The Google service account email address\nKeyFilePath - The full path to the .pl2 or .json key file. See here for more details.\nOAuthMechanism - Set to 0 to authenticate as a service account\n\n\ncon <- DBI::dbConnect(odbc::odbc(),\n                      Driver         = \"[your driver's name]\",\n                      Catalog        = \"[BigQuery project name]\",\n                      Email          = \"[Google service account email]\",\n                      KeyFilePath    = \"[Full path to key file]\",\n                      OAuthMechanism = 0)"
  },
  {
    "href": "databases/big-query.html#using-the-bigrquery-package",
    "title": "Google BigQuery",
    "section": "Using the bigrquery package",
    "text": "bigrquery is a database interface for R. This version is aimed at full compliance with the DBI specification. For more information, visit bigrquery’s official site: bigrquery.r-dbi.org\nTo install from CRAN:\ninstall.packages(\"bigrquery\")\nThe development version from github:\nTo install the development version:\n# install.packages(\"remotes\")\nremotes::install_github(\"r-dbi/DBI\")\nremotes::install_github(\"r-dbi/bigrquery\")\nTo connect:\nlibrary(DBI)\n\ncon <- dbConnect(\n  bigrquery::bigquery(),\n  project = \"publicdata\",\n  dataset = \"samples\",\n  billing = billing\n  )"
  },
  {
    "href": "databases/impala.html#driver-options",
    "title": "Apache Impala",
    "section": "Driver Options",
    "text": "Hadoop vendor - Download and install the driver made available by the Hadoop cluster provider (Cloudera, MapR, etc.). To locate the driver please consult the vendor’s website.\nRStudio Professional Drivers - RStudio Workbench (formerly RStudio Server Pro), RStudio Desktop Pro, RStudio Connect, or Shiny Server Pro users can download and use RStudio Professional Drivers at no additional charge. These drivers include an ODBC connector for Apache Impala. RStudio delivers standards-based, supported, professional ODBC drivers. Use RStudio Professional Drivers when you run R or Shiny with your production systems. See the RStudio Professional Drivers for more information."
  },
  {
    "href": "databases/impala.html#package-options",
    "title": "Apache Impala",
    "section": "Package Options",
    "text": "The odbc package, in combination with a driver, provides DBI support and an ODBC connection."
  },
  {
    "href": "databases/impala.html#connection-settings",
    "title": "Apache Impala",
    "section": "Connection Settings",
    "text": "There are six settings needed to make a connection:\n\nDriver - See the Drivers section for setup information\nHost - A network path to the database server\nSchema - The name of the schema\nUID - The user’s network ID or server local account\nPWD - The account’s password\nPort - Should be set to 21050\n\n\ncon <- DBI::dbConnect(odbc::odbc(),\n                      Driver = \"[your driver's name]\",\n                      Host   = \"[your server's path]\",\n                      Schema = \"[your schema's name]\",\n                      UID    = rstudioapi::askForPassword(\"Database user\"),\n                      PWD    = rstudioapi::askForPassword(\"Database password\"),\n                      Port   = 21050)"
  },
  {
    "href": "databases/impala.html#known-issues",
    "title": "Apache Impala",
    "section": "Known issues",
    "text": "Switching from Impala to Hive\nIf you create a table in Impala and then drop the Hive metadata, you will need to invalidate the Impala metadata.\nimpala_con <- dbConnect(odbc::odbc(), \"Impala\")\ndbWriteTable(impala_con, \"mtcars\", mtcars)\nhive_con <- dbConnect(odbc::odbc(), \"Hive\")\ndbRemoveTable(hive_con, \"mtcars\")\n\ndbReadTable(impala_con, \"mtcars\") # succeeds\ndbExistsTable(impala_con, \"mtcars\") # fails\n\ndbGetQuery(odbcCon, \"INVALIDATE METADATA mtcars\")\ndbExistsTable(impala_con, \"mtcars\") # succeeds\nThis happens because dropping the Hive metadata does not drop the Impala metadata. More information can be found in the Cloudera documentation here."
  },
  {
    "href": "databases/monetdb.html#installation",
    "title": "MonetDB",
    "section": "Installation",
    "text": "You can install the latest released version of MonetDBLite from CRAN with:\ninstall.packages(\"MonetDBLite\")\nThe latest development version from GitHub on the command line:\ngit clone https://github.com/hannesmuehleisen/MonetDBLite-R.git --depth 1 --recursive\nR CMD INSTALL MonetDBLite-R"
  },
  {
    "href": "databases/monetdb.html#usage",
    "title": "MonetDB",
    "section": "Usage",
    "text": "For information on how to use, and to report bugs, please refer to the package’s official GitHub repo: https://github.com/hannesmuehleisen/MonetDBLite-R"
  },
  {
    "href": "databases/sqlite.html#basic-usage",
    "title": "SQLite",
    "section": "Basic usage",
    "text": "library(DBI)\n# Create an ephemeral in-memory RSQLite database\ncon <- dbConnect(RSQLite::SQLite(), \":memory:\")\n\ndbListTables(con)\n## character(0)\ndbWriteTable(con, \"mtcars\", mtcars)\ndbListTables(con)\n## [1] \"mtcars\"\ndbListFields(con, \"mtcars\")\n##  [1] \"mpg\"  \"cyl\"  \"disp\" \"hp\"   \"drat\" \"wt\"   \"qsec\" \"vs\"   \"am\"   \"gear\"\n## [11] \"carb\"\ndbReadTable(con, \"mtcars\")\n##     mpg cyl  disp  hp drat    wt  qsec vs am gear carb\n## 1  21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4\n## 2  21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4\n## 3  22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\n## 4  21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1\n## 5  18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2\n## 6  18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1\n## 7  14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4\n## 8  24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\n## 9  22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2\n## 10 19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4\n## 11 17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4\n## 12 16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3\n## 13 17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3\n## 14 15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3\n## 15 10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4\n## 16 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4\n## 17 14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4\n## 18 32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\n## 19 30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\n## 20 33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\n## 21 21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1\n## 22 15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2\n## 23 15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2\n## 24 13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4\n## 25 19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2\n## 26 27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\n## 27 26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\n## 28 30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\n## 29 15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4\n## 30 19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6\n## 31 15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8\n## 32 21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2\n# You can fetch all results:\nres <- dbSendQuery(con, \"SELECT * FROM mtcars WHERE cyl = 4\")\ndbFetch(res)\n##     mpg cyl  disp  hp drat    wt  qsec vs am gear carb\n## 1  22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\n## 2  24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\n## 3  22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2\n## 4  32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\n## 5  30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\n## 6  33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\n## 7  21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1\n## 8  27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\n## 9  26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\n## 10 30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\n## 11 21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2\ndbClearResult(res)\n\n# Or a chunk at a time\nres <- dbSendQuery(con, \"SELECT * FROM mtcars WHERE cyl = 4\")\nwhile(!dbHasCompleted(res)){\n  chunk <- dbFetch(res, n = 5)\n  print(nrow(chunk))\n}\n## [1] 5\n## [1] 5\n## [1] 1\n# Clear the result\ndbClearResult(res)\n\n# Disconnect from the database\ndbDisconnect(con)"
  },
  {
    "href": "databases/sqlite.html#acknowledgements",
    "title": "SQLite",
    "section": "Acknowledgements",
    "text": "Many thanks to Doug Bates, Seth Falcon, Detlef Groth, Ronggui Huang, Kurt Hornik, Uwe Ligges, Charles Loboz, Duncan Murdoch, and Brian D. Ripley for comments, suggestions, bug reports, and/or patches."
  },
  {
    "href": "databases/mongodb.html#driver-options",
    "title": "MongoDB",
    "section": "Driver Options",
    "text": "MongoDB - Please refer to MongoDB’s website for instructions on how to download and setup their official driver: MongoDB ODBC driver page\nRStudio Professional Drivers - RStudio Workbench (formerly RStudio Server Pro), RStudio Desktop Pro, RStudio Connect, or Shiny Server Pro users can download and use RStudio Professional Drivers at no additional charge. These drivers include an ODBC connector for Apache Impala. RStudio delivers standards-based, supported, professional ODBC drivers. Use RStudio Professional Drivers when you run R or Shiny with your production systems. See the RStudio Professional Drivers for more information."
  },
  {
    "href": "databases/mongodb.html#package-options",
    "title": "MongoDB",
    "section": "Package Options",
    "text": "The odbc package, in combination with a driver, provides DBI support and an ODBC connection."
  },
  {
    "href": "databases/mongodb.html#connection-settings",
    "title": "MongoDB",
    "section": "Connection Settings",
    "text": "There are settings needed to make a connection:\n\nDriver - See the Drivers section for setup information\nServer - Server path or name\nPort - Should be set to 27017\nDatabase - Name of the database you wish to connect to\nAuthMechanism - See this article from MongoDB for more information: Authentication Mechanisms\nUID - The user’s network ID or server local account\nPWD - The account’s password\n\n\ncon <- DBI::dbConnect(\n      odbc::odbc(),\n      Driver        = \"[your driver's name]\",\n      Server        = \"[your server's path or name]\",\n      Port          = 27017,\n      Database      = \"[your database's name]\",\n      AuthMechanism = \"SCRAM-SHA-1\", # <- Example, depends server's auth setup\n      UID           = rstudioapi::askForPassword(\"Database user\"),\n      PWD           = rstudioapi::askForPassword(\"Database password\")\n      )"
  },
  {
    "href": "post/2017-05-02-bigrquery-1.html",
    "title": "bigrquery 1.0.0 Now on CRAN",
    "section": "",
    "text": "The new version of bigrquery is now available on CRAN. Here are some highlights:\n\nThe low-level API provides thin wrappers over the underlying REST API. In this version, all the low-level functions start with bq_, and mostly have the form bq_noun_verb(). This level of abstraction is most appropriate if you’re familiar with the REST API and you want do something not supported in the higher-level APIs.\nThe DBI interface wraps the low-level API and makes working with BigQuery like working with any other database system. This is the most convenient layer if you want to execute SQL queries in BigQuery or upload smaller amounts (i.e. <100 MB) of data.\nThe dplyr interface lets you treat BigQuery tables as if they are in-memory data frames. This is the most convenient layer if you don’t want to write SQL, but instead want dbplyr to write it for you.\n\nThe full article is available in the tidyverse.org site: https://www.tidyverse.org/articles/2018/04/bigrquery-1-0-0/"
  },
  {
    "href": "post/2017-11-04-databases-using-r-odsc-west-materials.html",
    "title": "Databases using R - ODSC West - Materials",
    "section": "",
    "text": "Materials from the Databases using R presentation at ODSC West. Link: https://github.com/edgararuiz/odsc-2017"
  },
  {
    "href": "post/2018-01-10-dbplyr-1-2-0-now-on-cran.html",
    "title": "dbplyr 1.2.0 Now on CRAN",
    "section": "",
    "text": "We are very excited to announce that dbplyr 1.2.0 is now available on CRAN! The full announcement was posted in tidyverse.org. Here are the highlights:\n\nNew custom translation for Microsoft Access and Teradata\nAmazon Redshift connections now point to the PostgreSQL translation.\nAdds support for two new database R packages. These new packages are fully DBI-compliant and tested with DBItest. We recommend to use these instead of older packages:\n\nRMariaDB, use in favor of RMySQL\nRPostgres, use in favor of RPostgreSQL\n\nROracle connections now point to the Oracle translation.\nCache query results using copy_to. This feature is very useful because after cached, the results of a long running query can be iteratively analyzed without having to wait for the same base query to run over and over.\nstringr functions: str_length(), str_to_upper(), str_to_lower(), str_replace_all(), str_detect(), and str_trim() are now supported.\nin_schema() should now work in more places, particularly in copy_to()\nFor those who need to extract the resulting SQL query from dplyr code, there is now a new function called remote_query(). Unlike show_query(), remote_query() returns a character object, and unlike sql_render(), the new function automatically detects the type of database attached to the given dplyr code and translates accordingly. There are four additional functions that provide a standard API to get metadata about a remote tbl, these are: remote_name(), remote_con(), remote_src(), and remote_query_plan().\nAdds support for temporary tables in Microsoft SQL Server. Additionally, the issue of certain operators working only in mutate() or only in filter() has been resolved.\nTwo new functions for developers: sql_aggregate() and win_aggregate(). They are for generating SQL and windowed SQL functions for aggregates."
  },
  {
    "href": "post/2019-03-05-modeldb-tidypredict.html#modeldb-0.1.2",
    "title": "New versions of modeldb and tidypredict now on CRAN",
    "section": "modeldb 0.1.2",
    "text": "Removes pipes and other dplyr dependencies from internal mlr() function\nConsolidates duplicated database operations in mlr()\nFixes an issue in simple_kmeans_db() when specifying variables"
  },
  {
    "href": "post/2019-03-05-modeldb-tidypredict.html#tidypredict-0.3.0",
    "title": "New versions of modeldb and tidypredict now on CRAN",
    "section": "tidypredict 0.3.0",
    "text": "New features\n\nAdds support for MARS models provided by the earth package\n\n\n\nImprovements\n\nNew parsed models are now list objects as opposed to data frames.\ntidypredict_to_column() no longer supports ranger and randomForest because of the multiple queries generated by multiple trees.\nAll functions that read the parsed models and create the tidy eval formula now use the list object.\nMost of the code that depends on dplyr programming has been removed.\nRemoves dependencies on: tidyr, tibble\n\n\n\nBug Fixes\n\nIt now returns all of the trees instead of just one for tree based models (randomForest & ranger) (#29)"
  },
  {
    "href": "post/2017-05-02-bigrquery-11.html",
    "title": "bigrquery 1.1.0 Now on CRAN",
    "section": "",
    "text": "The new version of bigrquery is now available on CRAN. This is a minor release, with some improved type support, and SQL translation. Please see the official release announcement in the tidyverse.org site: https://www.tidyverse.org/articles/2019/02/bigrquery-1-1-0/"
  },
  {
    "href": "post/2018-03-20-register-for-next-week-s-webinar.html",
    "title": "Register for next week’s webinar",
    "section": "",
    "text": "Title: Best Practices for Working with Databases When: March 28th at 11AM EDT By: Edgar Ruiz\nRegister here\nGet the most out of joining R forces with database forces. We will review key concepts, share the latest in R packages, and demo useful techniques. In this webinar, we will demonstrate a pragmatic approach for pairing R with databases. You will learn to use R’s familiar dplyr syntax to perform queries. We will also share the latest in R packages that aid with visualization and running predictions in-database. The webinar will focus on general principles and best practices; we will avoid technical details related to specific data store implementations."
  },
  {
    "href": "post/2017-11-29-dbplot-is-now-on-cran.html",
    "title": "dbplot is now on CRAN",
    "section": "",
    "text": "Leverages dplyr to process the calculations of a plot inside a database. This package provides helper functions that abstract the work at three levels: outputs a ggplot, outputs the calculations, outputs the formula needed to calculate bins.\n\n# You can install the released version from CRAN\ninstall.packages(\"dbplot\")\n\nFor more info, to report bugs, and request features please visit the package’s GitHub repo: https://github.com/edgararuiz/dbplot\nLink to the CRAN page: https://cran.r-project.org/web/packages/dbplot/index.html"
  },
  {
    "href": "post/updated-dashboards.html",
    "title": "Enterprise-ready Dashboards - New version",
    "section": "",
    "text": "An updated version of the Enterprise-ready Dashboards is now available. Here are the highlights:\n\nIncludes dashboard design principles\nIntroduces the r2d3 package for the visualizations\nProvides two new D3 visualizations that can be copied and easily integrated with other Shiny apps\nBetter handling of the base query using dplyr and shiny::reactive()\nAdds an example for handling values for Shiny inputs using the set_names() function.\nProvides a new, and single, GitHub repo to download all of the cde and material from one location."
  },
  {
    "href": "post/2017-11-30-introducing-drivers.html",
    "title": "Introducing Drivers Webinar - Materials",
    "section": "",
    "text": "Description:\nRStudio makes it easy to access and analyze your data with R. RStudio Professional Drivers are ODBC data connectors that help you connect to some of the most popular databases. Link: https://github.com/rstudio/webinars/tree/master/51-introducing-drivers\nThese drivers will help you:\n\nExplore your databases using the RStudio IDE\nDevelop and deploy Shiny applications that depend on databases\nUse databases with R in a production environment\n\nThis webinar will introduce you to our standards-based, supported, professional ODBC drivers. These are the drivers to use when you run R or Shiny with your production systems. It will also cover some of the most popular databases available today that we connect to including:\n\nMicrosoft SQL Server\nOracle\nPostgreSQL\nAmazon Redshift\nApache Hive\nApache Impala\nSalesforce"
  },
  {
    "href": "post/2018-02-28-tidypredict-0-2-0-now-on-cran.html",
    "title": "tidypredict 0.2.0 Now on CRAN",
    "section": "",
    "text": "Run predictions inside the database. tidypredict parses a fitted R model object, and returns a formula in ‘Tidy Eval’ code that calculates the predictions.\nIt works with several databases back-ends because it leverages dplyr and dbplyr for the final SQL translation of the algorithm. It currently supports lm(), glm(), randomForest() and ranger() models.\nMore information is available in the package’s official site: http://tidypredict.netlify.com/"
  },
  {
    "href": "post/2018-04-02-databases-r-webinar.html",
    "title": "Materials from webinar",
    "section": "",
    "text": "The video and materials are now available for the latest “R and Databases” webinar, presented by RStudio: https://www.rstudio.com/resources/videos/best-practices-for-working-with-databases-webinar/"
  },
  {
    "href": "post/2017-12-01-new-article-selecting-a-database-interface.html",
    "title": "Selecting a database interface - New article",
    "section": "",
    "text": "Background and recommendations on choosing between ODBC and JDBC, and between swtching from RODBC to odbc. Link: https://db.rstudio.com/best-practices/select-interface/"
  },
  {
    "href": "post/2018-04-09-select-schema.html",
    "title": "Schema selection - New article",
    "section": "",
    "text": "Background and recommendations to work with non-default schemas inside databases: Link to article"
  },
  {
    "href": "other-resources/media.html#modeling-in-databases-with-r",
    "title": "Media",
    "section": "Modeling in Databases with R",
    "text": "Edgar Ruiz - RStudio\nEvent: July 2019 Webinar\n\n\n\n\nModeling in Databases with R"
  },
  {
    "href": "other-resources/media.html#databases-using-r-the-latest",
    "title": "Media",
    "section": "Databases using R: The latest",
    "text": "Edgar Ruiz - RStudio\nEvent: January 2019 RStudio Conference 2019\n\n\n\n\nDatabases using R The latest - Edgar Ruiz"
  },
  {
    "href": "other-resources/media.html#best-practices-for-working-with-r-databases",
    "title": "Media",
    "section": "Best Practices for working with R & Databases",
    "text": "Edgar Ruiz - RStudio\nEvent: March 2018 Webinar\n\n\n\n\nBest practices for working with databases (March 2018)- Edgar Ruiz"
  },
  {
    "href": "other-resources/media.html#connecting-to-open-source-databases",
    "title": "Media",
    "section": "Connecting to open source databases",
    "text": "Kirill Muller\nEvent: rstudio::conf2018\n\n\n\n\nConnecting to open source databases – Kirill Muller"
  },
  {
    "href": "other-resources/media.html#best-practices-for-working-with-databases",
    "title": "Media",
    "section": "Best practices for working with databases",
    "text": "by Edgar Ruiz - RStudio\nEvent: rstudio::conf2018\n\n\n\n\nBest practices for working with databases – Edgar Ruiz"
  },
  {
    "href": "other-resources/media.html#odbc---a-modern-database-interface",
    "title": "Media",
    "section": "odbc - A modern database interface",
    "text": "by Jim Hester - RStudio\nEvent: useR!2017"
  },
  {
    "href": "other-resources/media.html#interacting-with-databases-from-shiny",
    "title": "Media",
    "section": "Interacting with databases from Shiny",
    "text": "by Barbara Borges Ribeiro - RStudio\nEvent: useR!2017"
  },
  {
    "href": "other-resources/media.html#implyr-a-dplyr-backend-for-apache-impala",
    "title": "Media",
    "section": "implyr: A dplyr backend for Apache Impala",
    "text": "by Ian Cook - Cloudera\nEvent: useR!2017"
  },
  {
    "href": "other-resources/media.html#improving-dbi",
    "title": "Media",
    "section": "Improving DBI",
    "text": "by Kirill Müller\nEvent: useR!2017"
  },
  {
    "href": "r-packages/dplyr.html#getting-started",
    "title": "Using dplyr with databases",
    "section": "Getting started",
    "text": "To use databases with dplyr, you need to first install dbplyr:\n\ninstall.packages(\"dbplyr\")\n\nYou’ll also need to install a DBI backend package. The DBI package provides a common interface that allows dplyr to work with many different databases using the same code. DBI is automatically installed with dbplyr, but you need to install a specific backend for the database that you want to connect to.\nFive commonly used backends are:\n\nRMySQL connects to MySQL and MariaDB\nRPostgreSQL connects to Postgres and Redshift.\nRSQLite embeds a SQLite database.\nodbc connects to many commercial databases via the open database connectivity protocol.\nbigrquery connects to Google’s BigQuery.\n\nIf the database you need to connect to is not listed here, you’ll need to do some investigation yourself.\nIn this vignette, we’re going to use the RSQLite backend, which is automatically installed when you install dbplyr. SQLite is a great way to get started with databases because it’s completely embedded inside an R package. Unlike most other systems, you don’t need to set up a separate database server. SQLite is great for demos, but is surprisingly powerful, and with a little practice you can use it to easily work with many gigabytes of data."
  },
  {
    "href": "r-packages/dplyr.html#connecting-to-the-database",
    "title": "Using dplyr with databases",
    "section": "Connecting to the database",
    "text": "To work with a database in dplyr, you must first connect to it, using DBI::dbConnect(). We’re not going to go into the details of the DBI package here, but it’s the foundation upon which dbplyr is built. You’ll need to learn more about if you need to do things to the database that are beyond the scope of dplyr.\n\nlibrary(dplyr)\ncon <- DBI::dbConnect(RSQLite::SQLite(), path = \":dbname:\")\n\nThe arguments to DBI::dbConnect() vary from database to database, but the first argument is always the database backend. It’s RSQLite::SQLite() for RSQLite, RMySQL::MySQL() for RMySQL, RPostgreSQL::PostgreSQL() for RPostgreSQL, odbc::odbc() for odbc, and bigrquery::bigquery() for BigQuery. SQLite only needs one other argument: the path to the database. Here we use the special string, \":memory:\", which causes SQLite to make a temporary in-memory database.\nMost existing databases don’t live in a file, but instead live on another server. In real life that your code will look more like this:\n\ncon <- DBI::dbConnect(RMySQL::MySQL(), \n  host = \"database.rstudio.com\",\n  user = \"hadley\",\n  password = rstudioapi::askForPassword(\"Database password\")\n)\n\n(If you’re not using RStudio, you’ll need some other way to securely retrieve your password. You should never record it in your analysis scripts or type it into the console.)\nOur temporary database has no data in it, so we’ll start by copying over nycflights13::flights using the convenient copy_to() function. This is a quick and dirty way of getting data into a database and is useful primarily for demos and other small jobs.\n\ncopy_to(con, nycflights13::flights, \"flights\",\n  temporary = FALSE, \n  indexes = list(\n    c(\"year\", \"month\", \"day\"), \n    \"carrier\", \n    \"tailnum\",\n    \"dest\"\n  )\n)\n\nAs you can see, the copy_to() operation has an additional argument that allows you to supply indexes for the table. Here we set up indexes that will allow us to quickly process the data by day, carrier, plane, and destination. Creating the write indices is key to good database performance, but is unfortunately beyond the scope of this article.\nNow that we’ve copied the data, we can use tbl() to take a reference to it:\n\nflights_db <- tbl(con, \"flights\")\n\nWhen you print it out, you’ll notice that it mostly looks like a regular tibble:\n\nflights_db \n\n#> # Source:   table<flights> [?? x 19]\n#> # Database: sqlite 3.35.5 []\n#>    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n#>   <int> <int> <int>    <int>          <int>     <dbl>    <int>          <int>\n#> 1  2013     1     1      517            515         2      830            819\n#> 2  2013     1     1      533            529         4      850            830\n#> 3  2013     1     1      542            540         2      923            850\n#> 4  2013     1     1      544            545        -1     1004           1022\n#> 5  2013     1     1      554            600        -6      812            837\n#> 6  2013     1     1      554            558        -4      740            728\n#> # … with more rows, and 11 more variables: arr_delay <dbl>, carrier <chr>,\n#> #   flight <int>, tailnum <chr>, origin <chr>, dest <chr>, air_time <dbl>,\n#> #   distance <dbl>, hour <dbl>, minute <dbl>, time_hour <dbl>\n\n\nThe main difference is that you can see that it’s a remote source in a SQLite database."
  },
  {
    "href": "r-packages/dplyr.html#generating-queries",
    "title": "Using dplyr with databases",
    "section": "Generating queries",
    "text": "To interact with a database you usually use SQL, the Structured Query Language. SQL is over 40 years old, and is used by pretty much every database in existence. The goal of dbplyr is to automatically generate SQL for you so that you’re not forced to use it. However, SQL is a very large language, and dbplyr doesn’t do everything. It focuses on SELECT statements, the SQL you write most often as an analyst.\nMost of the time you don’t need to know anything about SQL, and you can continue to use the dplyr verbs that you’re already familiar with:\n\nflights_db %>% select(year:day, dep_delay, arr_delay)\n\n#> # Source:   lazy query [?? x 5]\n#> # Database: sqlite 3.35.5 []\n#>    year month   day dep_delay arr_delay\n#>   <int> <int> <int>     <dbl>     <dbl>\n#> 1  2013     1     1         2        11\n#> 2  2013     1     1         4        20\n#> 3  2013     1     1         2        33\n#> 4  2013     1     1        -1       -18\n#> 5  2013     1     1        -6       -25\n#> 6  2013     1     1        -4        12\n#> # … with more rows\n\n\nflights_db %>% filter(dep_delay > 240)\n\n#> # Source:   lazy query [?? x 19]\n#> # Database: sqlite 3.35.5 []\n#>    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n#>   <int> <int> <int>    <int>          <int>     <dbl>    <int>          <int>\n#> 1  2013     1     1      848           1835       853     1001           1950\n#> 2  2013     1     1     1815           1325       290     2120           1542\n#> 3  2013     1     1     1842           1422       260     1958           1535\n#> 4  2013     1     1     2115           1700       255     2330           1920\n#> 5  2013     1     1     2205           1720       285       46           2040\n#> 6  2013     1     1     2343           1724       379      314           1938\n#> # … with more rows, and 11 more variables: arr_delay <dbl>, carrier <chr>,\n#> #   flight <int>, tailnum <chr>, origin <chr>, dest <chr>, air_time <dbl>,\n#> #   distance <dbl>, hour <dbl>, minute <dbl>, time_hour <dbl>\n\n\nflights_db %>% \n  group_by(dest) %>%\n  summarise(delay = mean(dep_time))\n\n#> Warning: Missing values are always removed in SQL.\n#> Use `mean(x, na.rm = TRUE)` to silence this warning\n#> This warning is displayed only once per session.\n\n\n#> # Source:   lazy query [?? x 2]\n#> # Database: sqlite 3.35.5 []\n#>   dest  delay\n#>   <chr> <dbl>\n#> 1 ABQ   2006.\n#> 2 ACK   1033.\n#> 3 ALB   1627.\n#> 4 ANC   1635.\n#> 5 ATL   1293.\n#> 6 AUS   1521.\n#> # … with more rows\n\n\nHowever, in the long run, I highly recommend you at least learn the basics of SQL. It’s a valuable skill for any data scientist, and it will help you debug problems if you run into problems with dplyr’s automatic translation. If you’re completely new to SQL, you might start with this codeacademy tutorial. If you have some familiarity with SQL and you’d like to learn more, I found how indexes work in SQLite and 10 easy steps to a complete understanding of SQL to be particularly helpful.\nThe most important difference between ordinary data frames and remote database queries is that your R code is translated into SQL and executed in the database, not in R. When working with databases, dplyr tries to be as lazy as possible:\n\nIt never pulls data into R unless you explicitly ask for it.\nIt delays doing any work until the last possible moment: it collects together everything you want to do and then sends it to the database in one step.\n\nFor example, take the following code:\n\ntailnum_delay_db <- flights_db %>% \n  group_by(tailnum) %>%\n  summarise(\n    delay = mean(arr_delay),\n    n = n()\n  ) %>% \n  arrange(desc(delay)) %>%\n  filter(n > 100)\n\nSurprisingly, this sequence of operations never touches the database. It’s not until you ask for the data (e.g., by printing tailnum_delay) that dplyr generates the SQL and requests the results from the database. Even then it tries to do as little work as possible and only pulls down a few rows.\n\ntailnum_delay_db\n\n#> Warning: ORDER BY is ignored in subqueries without LIMIT\n#> ℹ Do you need to move arrange() later in the pipeline or use window_order() instead?\n\n\n#> # Source:     lazy query [?? x 3]\n#> # Database:   sqlite 3.35.5 []\n#> # Ordered by: desc(delay)\n#>   tailnum delay     n\n#>   <chr>   <dbl> <int>\n#> 1 <NA>    NA     2512\n#> 2 N0EGMQ   9.98   371\n#> 3 N10156  12.7    153\n#> 4 N10575  20.7    289\n#> 5 N11106  14.9    129\n#> 6 N11107  15.0    148\n#> # … with more rows\n\n\nBehind the scenes, dplyr is translating your R code into SQL. You can see the SQL it’s generating with show_query():\n\ntailnum_delay_db %>% show_query()\n\n#> Warning: ORDER BY is ignored in subqueries without LIMIT\n#> ℹ Do you need to move arrange() later in the pipeline or use window_order() instead?\n\n\n#> <SQL>\n#> SELECT *\n#> FROM (SELECT `tailnum`, AVG(`arr_delay`) AS `delay`, COUNT(*) AS `n`\n#> FROM `flights`\n#> GROUP BY `tailnum`)\n#> WHERE (`n` > 100.0)\n\n\nIf you’re familiar with SQL, this probably isn’t exactly what you’d write by hand, but it does the job. You can learn more about the SQL translation in vignette(\"sql-translation\").\nTypically, you’ll iterate a few times before you figure out what data you need from the database. Once you’ve figured it out, use collect() to pull all the data down into a local tibble:\n\ntailnum_delay <- tailnum_delay_db %>% collect()\n\n#> Warning: ORDER BY is ignored in subqueries without LIMIT\n#> ℹ Do you need to move arrange() later in the pipeline or use window_order() instead?\n\ntailnum_delay\n\n#> # A tibble: 1,201 x 3\n#>   tailnum delay     n\n#>   <chr>   <dbl> <int>\n#> 1 <NA>    NA     2512\n#> 2 N0EGMQ   9.98   371\n#> 3 N10156  12.7    153\n#> 4 N10575  20.7    289\n#> 5 N11106  14.9    129\n#> 6 N11107  15.0    148\n#> # … with 1,195 more rows\n\n\ncollect() requires that database does some work, so it may take a long time to complete. Otherwise, dplyr tries to prevent you from accidentally performing expensive query operations:\n\nBecause there’s generally no way to determine how many rows a query will return unless you actually run it, nrow() is always NA.\nBecause you can’t find the last few rows without executing the whole query, you can’t use tail().\n\n\nnrow(tailnum_delay_db)\n\n#> [1] NA\n\n\ntail(tailnum_delay_db)\n\n#> Error: tail() is not supported by sql sources\n\n\nYou can also ask the database how it plans to execute the query with explain(). The output is database-dependent and can be esoteric, but learning a bit about it can be very useful because it helps you understand if the database can execute the query efficiently, or if you need to create new indices."
  },
  {
    "href": "r-packages/dplyr.html#creating-your-own-database",
    "title": "Using dplyr with databases",
    "section": "Creating your own database",
    "text": "If you don’t already have a database, here’s some advice from my experiences setting up and running all of them. SQLite is by far the easiest to get started with, but the lack of window functions makes it limited for data analysis. PostgreSQL is not too much harder to use and has a wide range of built-in functions. In my opinion, you shouldn’t bother with MySQL/MariaDB; it’s a pain to set up, the documentation is sub par, and it’s less feature-rich than Postgres. Google BigQuery might be a good fit if you have very large data, or if you’re willing to pay (a small amount of) money to someone who’ll look after your database.\nAll of these databases follow a client-server model - a computer that connects to the database and the computer that is running the database (the two may be one and the same, but usually aren’t). Getting one of these databases up and running is beyond the scope of this article, but there are plenty of tutorials available on the web.\n\nMySQL/MariaDB\nIn terms of functionality, MySQL lies somewhere between SQLite and PostgreSQL. It provides a wider range of built-in functions, but it does not support window functions (so you can’t do grouped mutates and filters).\n\n\nPostgreSQL\nPostgreSQL is a considerably more powerful database than SQLite. It has:\n\na much wider range of built-in functions, and\nsupport for window functions, which allow grouped subset and mutates to work.\n\n\n\nBigQuery\nBigQuery is a hosted database server provided by Google. To connect, you need to provide your project, dataset and optionally a project for billing (if billing for project isn’t enabled).\nIt provides a similar set of functions to Postgres and is designed specifically for analytic workflows. Because it’s a hosted solution, there’s no setup involved, but if you have a lot of data, getting it to Google can be an ordeal (especially because upload support from R is not great currently). (If you have lots of data, you can ship hard drives!)"
  },
  {
    "href": "r-packages/pool.html#usage",
    "title": "Pooling database connections in R",
    "section": "Usage",
    "text": "Here’s a simple example of using a pool within a Shiny app (feel free to try it yourself):\nlibrary(shiny)\nlibrary(dplyr)\nlibrary(pool)\n\npool <- dbPool(\n  drv = RMySQL::MySQL(),\n  dbname = \"shinydemo\",\n  host = \"shiny-demo.csa7qlmguqrf.us-east-1.rds.amazonaws.com\",\n  username = \"guest\",\n  password = \"guest\"\n)\nonStop(function() {\n  poolClose(pool)\n})\n  \nui <- fluidPage(\n  textInput(\"ID\", \"Enter your ID:\", \"5\"),\n  tableOutput(\"tbl\"),\n  numericInput(\"nrows\", \"How many cities to show?\", 10),\n  plotOutput(\"popPlot\")\n)\n\nserver <- function(input, output, session) {\n  output$tbl <- renderTable({\n    pool %>% tbl(\"City\") %>% filter(ID == !!input$ID) %>% collect()\n  })\n  output$popPlot <- renderPlot({\n    df <- pool %>% tbl(\"City\") %>% head(input$nrows) %>% collect()\n    pop <- df$Population\n    names(pop) <- df$Name\n    barplot(pop)\n  })\n}\n\nshinyApp(ui, server)\nNote: if you prefer to connect using an ODBC driver and a data source name (DSN), you can use the odbc package along with the dsn argument.\npool <- dbPool(odbc::odbc(), dsn = \"myDSN\")"
  },
  {
    "href": "r-packages/pool.html#concept",
    "title": "Pooling database connections in R",
    "section": "Concept",
    "text": "The pool package adds a new level of abstraction when connecting to a database: instead of directly fetching a connection from the database, you will create an object (called a pool) with a reference to that database. The pool holds a number of connections to the database. Some of these may be currently in-use and some of these may be idle, waiting for a query to request them. Each time you make a query, you are querying the pool, rather than the database. Under the hood, the pool will either give you an idle connection that it previously fetched from the database or, if it has no free connections, fetch one and give it to you. You never have to create or close connections directly: the pool knows when it should grow, shrink or keep steady. You only need to close the pool when you’re done."
  },
  {
    "href": "r-packages/pool.html#context-and-motivation",
    "title": "Pooling database connections in R",
    "section": "Context and motivation",
    "text": "When you’re connecting to a database, it is important to manage your connections: when to open them (taking into account that this is a potentially long process for remote databases), how to keep track of them, and when to close them. This is always true, but it becomes especially relevant for Shiny apps, where not following best practices can lead to many slowdowns (from inadvertantly opening too many connections) and/or many leaked connections (i.e. forgetting to close connections once you no longer need them). Over time, leaked connections could accumulate and substantially slow down your app, as well as overwhelming the database itself.\nOversimplifying a bit, we can think of connection management in Shiny as a spectrum from the extreme of just having one connection per app (potentially serving several sessions of the app) to the extreme of opening (and closing) one connection for each query you make. Neither of these approaches is great. You can expand either of the arrows below to see the source code for each extreme, but that is not essential to understading the problems described below.\n\n\noneConnectionPerApp.R\n\nlibrary(shiny)\nlibrary(dplyr)\nlibrary(DBI)\n\nconn <- dbConnect(\n    drv = RMySQL::MySQL(),\n    dbname = \"shinydemo\",\n    host = \"shiny-demo.csa7qlmguqrf.us-east-1.rds.amazonaws.com\",\n    username = \"guest\",\n    password = \"guest\"\n  )\nonStop(function() {\n  dbDisconnect(conn)\n})\n  \nui <- fluidPage(\n  textInput(\"ID\", \"Enter your ID:\", \"5\"),\n  tableOutput(\"tbl\"),\n  numericInput(\"nrows\", \"How many cities to show?\", 10),\n  plotOutput(\"popPlot\")\n)\n  \nserver <- function(input, output, session) {\n  output$tbl <- renderTable({\n    conn %>% tbl(\"City\") %>% filter(ID == input$ID) %>% collect()\n  })\n  output$popPlot <- renderPlot({\n    df <- conn %>% tbl(\"City\") %>% head(input$nrows) %>% collect()\n    pop <- df$Population\n    names(pop) <- df$Name\n    barplot(pop)\n  })\n}\n  \nshinyApp(ui, server)\n\n\n\noneConnectionPerQuery.R\n\nlibrary(shiny)\nlibrary(dplyr)\nlibrary(DBI)\n\nargs <- list(\n  drv = RMySQL::MySQL(),\n  dbname = \"shinydemo\",\n  host = \"shiny-demo.csa7qlmguqrf.us-east-1.rds.amazonaws.com\",\n  username = \"guest\",\n  password = \"guest\"\n)\n  \nui <- fluidPage(\n  textInput(\"ID\", \"Enter your ID:\", \"5\"),\n  tableOutput(\"tbl\"),\n  numericInput(\"nrows\", \"How many cities to show?\", 10),\n  plotOutput(\"popPlot\")\n)\n  \nserver <- function(input, output, session) {\n  output$tbl <- renderTable({\n    conn <- do.call(dbConnect, args)\n    on.exit(dbDisconnect(conn))\n    \n    conn %>% tbl(\"City\") %>% filter(ID == input$ID) %>% collect()\n  })\n  output$popPlot <- renderPlot({\n    conn <- do.call(dbConnect, args)\n    on.exit(dbDisconnect(conn))\n    \n    df <- conn %>% tbl(\"City\") %>% head(input$nrows) %>% collect()\n    pop <- df$Population\n    names(pop) <- df$Name\n    barplot(pop)\n  })\n}\n  \nshinyApp(ui, server)\n\n\nOpening only one connection per app makes it fast (because, in the whole app, you only fetch one connection) and your code is kept as simple as possible. However:\n\nit cannot handle simultaneous requests (e.g. two sessions open, both querying the database at the same time);\nif the connection breaks at some point (maybe the database server crashed), you won’t get a new connection (you have to exit the app and re-run it);\nfinally, if you are not quite at this extreme, and you use more than one connection per app (but fewer than one connection per query), it can be difficult to keep track of all your connections, since you’ll be opening and closing them in potentially very different places.\n\nWhile the other extreme of opening (and closing) one connection for each query you make resolves all of these points, it is terribly slow (each time we need to access the database, we first have to fetch a connection), and you need a lot more (boilerplate) code to connect and disconnect the connection within each reactive/function.\nThe pool package was created so you don’t have to worry about this at all. Since pool abstracts away the logic of connection management, for the vast majority of cases, you never have to deal with connections directly. Since the pool “knows” when it should have more connections and how to manage them, you have all the advantages of the second approach (one connection per query), without the disadvantages. You are still using one connection per query, but that connection is always fetched and returned to the pool, rather than getting it from the database directly. This is a whole lot faster and more efficient. Finally, the code is kept just as simple as the code in the first approach (only one connection for the entire app). In fact, if you look back at the pool Shiny app example above, you will notice that the code structure is essentially the same that you’d use to open a connection at the start of an app and close it at the end."
  },
  {
    "href": "r-packages/DBI.html#class-structure",
    "title": "Using DBI",
    "section": "Class structure",
    "text": "There are four main DBI classes, three of which are each extended by individual database backends:\n\nDBIObject: a common base class for all DBI.\nDBIDriver: a base class representing overall DBMS properties. Typically generator functions instantiate the driver objects like RSQLite(), RPostgreSQL(), RMySQL() etc.\nDBIConnection: represents a connection to a specific database\nDBIResult: the result of a DBMS query or statement.\n\nAll classes are virtual: they cannot be instantiated directly and instead must be subclassed."
  },
  {
    "href": "r-packages/DBI.html#history",
    "title": "Using DBI",
    "section": "History",
    "text": "The following history of DBI was contributed by David James, the driving force behind the development of DBI, and many of the packages that implement it.\nThe idea/work of interfacing S (originally S3 and S4) to RDBMS goes back to the mid- and late 1990’s in Bell Labs. The first toy interface I did was to implement John Chamber’s early concept of “Data Management in S” (1991). The implementation followed that interface pretty closely and immediately showed some of the limitations when dealing with very large databases; if my memory serves me, the issue was the instance-based of the language back then, e.g., if you attached an RDBMS to the search() path and then needed to resolve a symbol “foo”, you effectively had to bring all the objects in the database to check their mode/class, i.e., the instance object had the metadata in itself as attributes. The experiment showed that the S3 implementation of “data management” was not really suitable to large external RDBMS (probably it was never intended to do that anyway). (Note however, that since then, John and Duncan Temple Lang generalized the data management in S4 a lot, including Duncan’s implementation in his RObjectTables package where he considered a lot of synchronization/caching issues relevant to DBI and, more generally, to most external interfaces).\nBack then we were working very closely with Lucent’s microelectronics manufacturing — our colleagues there had huge Oracle (mostly) databases that we needed to constantly query via SQL*Plus. My colleague Jake Luciani was developing advanced applications in C and SQL, and the two of us came up with the first implementation of S3 directly connecting with Oracle. What I remember is that the Linux PRO*C pre-compiler (that embedded SQL in C code) was very buggy — we spent a lot of time looking for workarounds and tricks until we got the C interface running. At the time, other projects within Bell Labs began using MySQL, and we moved to MySQL (with the help of Doug Bates’ student Saikat DebRoy, then a summer intern) with no intentions of looking back at the very difficult Oracle interface. It was at this time that I moved all the code from S3 methods to S4 classes and methods and begun reaching out to the S/R community for suggestions, ideas, etc. All (most) of this work was on Bell Labs versions of S3 and S4, but I made sure it worked with S-Plus. At some point around 2000 (I don’t remember exactly when), I ported all the code to R regressing to S3 methods, and later on (once S4 classes and methods were available in R) I re-implemented everything back to S4 classes and methods in R (a painful back-and-forth). It was at this point that I decided to drop S-Plus altogether. Around that time, I came across a very early implementation of SQLite and I was quite interested and thought it was a very nice RDBMS that could be used for all kinds of experimentation, etc., so it was pretty easy to implement on top of the DBI.\nWithin the R community, there were quite a number of people that showed interest on defining a common interface to databases, but only a few folks actually provided code/suggestions/etc. (Tim Keitt was most active with the dbi/PostgreSQL packages — he also was considering what he called “proxy” objects, which was reminiscent of what Duncan had been doing). Kurt Hornick, Vincent Carey, Robert Gentleman, and others provided suggestions/comments/support for the DBI definition. By around 2003, the DBI was more or less implemented as it is today.\nI’m sure I’ll forget some (most should be in the THANKS sections of the various packages), but the names that come to my mind at this moment are Jake Luciani (ROracle), Don MacQueen and other early ROracle users (super helpful), Doug Bates and his student Saikat DebRoy for RMySQL, Fei Chen (at the time a student of Prof. Ripley) also contributed to RMySQL, Tim Keitt (working on an early S3 interface to PostgrSQL), Torsten Hothorn (worked with mSQL and also MySQL), Prof. Ripley working/extending the RODBC package, in addition to John Chambers and Duncan Temple-Lang who provided very important comments and suggestions.\nActually, the real impetus behind the DBI was always to do distributed statistical computing — not to provide a yet-another import/export mechanism — and this perspective was driven by John and Duncan’s vision and work on inter-system computing, COM, CORBA, etc. I’m not sure many of us really appreciated (even now) the full extent of those ideas and concepts. Just like in other languages (C’s ODBC, Java’s JDBC, Perl’s DBI/DBD, Python dbapi), R/S DBI was meant to unify the interfacing to RDBMS so that R/S applications could be developed on top of the DBI and not be hard coded to any one relation database. The interface I tried to follow the closest was the Python’s DBAPI — I haven’t worked on this topic for a while, but I still feel Python’s DBAPI is the cleanest and most relevant for the S language."
  },
  {
    "href": "r-packages/odbc.html#using",
    "title": "Using an ODBC driver",
    "section": "Using",
    "text": "All of the following examples assume you have already created a connection called con. To find out how to connect to your specific database type, please visit the Databases page.\n\nDatabase information\nThe odbc package gives you tools to explore objects and columns in the database.\n# Top level objects\nodbcListObjects(con)\n\n# Tables in a schema\nodbcListObjects(con, catalog=\"mydb\", schema=\"dbo\")\n\n# Columns in a table\nodbcListColumns(con, catalog=\"mydb\", schema=\"dbo\", table=\"cars\")\n\n# Database structure\nodbcListObjectTypes(con)\nYou can also see other data sources and drivers on the system.\n# All data sources\nodbcListDataSources()\n\n# All drivers\nodbcListDrivers()\n\n\nReading and writing tables\nThe DBI package has functions for reading and writing tables. dbWriteTable() will write an R data frame to a SQL table. dbReadTable() will read a SQL table into an R data frame.\ndbWriteTable(con, \"cars\", cars)\ndbReadTable(con, \"cars\")\nYou can specify tables outside the database with the Id() command.\ntable_id <- Id(catalog = \"mydb\", schema = \"dbo\", table = \"cars\")\ndbReadTable(con, table_id)\n\n\nQueries and statements\nFor interactive queries, use dbGetQuery() to submit a query and fetch the results. To fetch the results separately, use dbSendQuery() and dbFetch(). The n= argument in dbFetch() can be used to fetch partial results.\n# Return the results for an arbitrary query\ndbGetQuery(con, \"SELECT speed, dist FROM cars\")\n\n# Fetch the first 100 records\nquery <- dbSendQuery(con, \"SELECT speed, dist FROM cars\")\ndbFetch(query, n = 10)\ndbClearResult(query)\nYou can execute arbitrary SQL statements with dbExecute(). Note: many database API’s distinguish between direct and prepared statements. If you want to force a direct statement (for example, if you want to create a local temp table in Microsoft SQL Server), then pass immdediate=TRUE.\ndbExecute(con, \"INSERT INTO cars (speed, dist) VALUES (88, 30)\")\ndbExecute(con, \"CREATE TABLE #cars_tmp (speed int, dist int)\", immediate = TRUE)"
  },
  {
    "href": "r-packages/odbc.html#odbc-performance-benchmarks",
    "title": "Using an ODBC driver",
    "section": "odbc Performance Benchmarks",
    "text": "The odbc package is often much faster than the existing RODBC and DBI compatible RODBCDBI packages. The tests below were carried out on PostgreSQL and Microsoft SQL Server using the nycflights13::flights dataset (336,776 rows, 19 columns).\n\nPostgreSQL Results\n\n\n\nPackage\nFunction\nUser\nSystem\nElapsed\n\n\n\n\nodbc\nReading\n5.119\n0.290\n6.771\n\n\nRODBCDBI\nReading\n19.203\n1.356\n21.724\n\n\nodbc\nWriting\n7.802\n3.703\n26.016\n\n\nRODBCDBI\nWriting\n6.693\n3.786\n48.423\n\n\n\nlibrary(DBI)\nlibrary(RODBCDBI)\nlibrary(tibble)\n\nodbc <- dbConnect(odbc::odbc(), dsn = \"PostgreSQL\")\nrodbc <- dbConnect(RODBCDBI::ODBC(), dsn = \"PostgreSQL\")\n\n# odbc Reading\nsystem.time(odbc_result <- dbReadTable(odbc, \"flights\"))\n\n# RODBCDBI Reading\nsystem.time(rodbc_result <- dbReadTable(rodbc, \"flights\"))\n\n# odbc Reading\nsystem.time(dbWriteTable(odbc, \"flights3\", as.data.frame(flights)))\n\n# RODBCDBI Writing (note: rodbc does not support writing timestamps natively)\nsystem.time(dbWriteTable(rodbc, \"flights2\", as.data.frame(flights[, names(flights) != \"time_hour\"])))\n\n\nMicrosoft SQL Server Results\n\n\n\nPackage\nFunction\nUser\nSystem\nElapsed\n\n\n\n\nodbc\nReading\n2.187\n0.108\n2.298\n\n\nRSQLServer\nReading\n5.101\n1.289\n3.584\n\n\nodbc\nWriting\n12.336\n0.412\n21.802\n\n\nRSQLServer\nWriting\n645.219\n12.287\n820.806\n\n\n\nlibrary(\"RSQLServer\")\nrsqlserver <- dbConnect(RSQLServer::SQLServer(), server = \"SQLServer\")\nodbc <- dbConnect(odbc::odbc(), dsn = \"PostgreSQL\")\n\n# odbc Reading\nsystem.time(dbReadTable(odbc, \"flights\", as.data.frame(flights)))\n\n# RSQLServer Reading\nsystem.time(dbReadTable(rsqlserver, \"flights\", as.data.frame(flights)))\n\n# odbc Writing\nsystem.time(dbWriteTable(odbc, \"flights3\", as.data.frame(flights)))\n\n# RSQLServer Writing\nsystem.time(dbWriteTable(rsqlserver, \"flights2\", as.data.frame(flights)))"
  },
  {
    "href": "getting-started/overview.html#dplyr-as-a-database-interface",
    "title": "Databases using R",
    "section": "dplyr as a database interface",
    "text": "The dplyr package simplifies data transformation. It provides a consistent set of functions, called verbs, that can be used in succession and interchangeably to gain understanding of the data iteratively.\ndplyr is able to interact with databases directly by translating the dplyr verbs into SQL queries. This convenient feature allows you to ‘speak’ directly with the database from R. Other advantages of this approach are:\n\n\n\n\nRun data exploration routines over all of the data, instead of importing part of the data into R.\nUse the SQL Engine to run the data transformations. In effect, computation is being pushed to the database.\nCollect into R only a targeted dataset.\nAll of your code is in R. Becausedplyr is used to communicate with the database, there is no need to alternate between languages or tools to perform the data exploration."
  },
  {
    "href": "getting-started/overview.html#connect-to-a-database",
    "title": "Databases using R",
    "section": "Connect to a database",
    "text": "At the center of this approach is the DBI package. This package acts as ‘middle-ware’ between packages to allow connectivity with the database from the user or other packages. It provides a consistent set of functions regardless of the database type being accessed. The dplyr package depends on the DBI package for communication with databases.\nThere are packages that enables a direct connection between the an open-source database and R. Currently, such packages exist for the following databases: MySQL, SQLite, PostgreSQL, and bigquery.\n\n\n\nMost commercial databases, like Oracle and Microsoft SQL Server, offer ODBC drivers that allow you to connect your tool to the database. Even though there are R packages that allow you to use ODBC drivers, the connection will most likely not be compatible with DBI. The new odbc package solves that problem by providing a DBI backend to any ODBC driver connection.\nIf you are interested in creating your own package that connects DBI to a database, please review the article DBI Backend."
  },
  {
    "href": "getting-started/overview.html#sql-translations-for-dplyr",
    "title": "Databases using R",
    "section": "SQL Translations for dplyr",
    "text": "A complementary package called dbplyr contains the translations of the vendor-specific SQL for dplyr to use. A list of known supported databases are available in our Databases page.\nIs the database you are interested in not listed here? You can still use DBI and odbc to connect and send SQL queries. If you would like to contribute a translation, please see the SQL Translation page in this website."
  },
  {
    "href": "getting-started/overview.html#example",
    "title": "Databases using R",
    "section": "Example",
    "text": "The same dplyr syntax used with data in R will also work with data in a database. In the example below, data from the nycflights13 package are loaded into a SQLite database then queried from R. The results from the query are then collected into R and visualized with ggplot2. The process is the same if you are using an enterprise data warehouse — like Microsoft SQL Server or Snowflake’s data cloud.\nlibrary(DBI)\nlibrary(dplyr)\nlibrary(ggplot2)\n\n# SQLite database\ncon <- dbConnect(RSQLite::SQLite(), \":memory:\")\ncopy_to(con, nycflights13::flights, \"FLIGHTS\")\n\n# ODBC databases (requires a live database connection)\n#con <- dbConnect(odbc::odbc(), \"SQL Server\")\n#con <- dbConnect(odbc::odbc(), \"Snowflake\")\n\n# Query, collect results, and visualize\ntbl(con, \"FLIGHTS\") %>%\n  filter(distance > 75) %>%\n  group_by(origin, hour) %>%\n  summarise(delay = mean(dep_delay, na.rm = TRUE)) %>%\n  collect() %>%\n  ggplot(aes(hour, delay, color = origin)) + geom_line()"
  },
  {
    "href": "getting-started/database-queries.html",
    "title": "Database Queries With R",
    "section": "",
    "text": "There are many ways to query data with R. This article shows you three of the most common ways:\n\nUsing DBI\nUsing dplyr syntax\nUsing R Notebooks\n\n\nBackground\nSeveral recent package improvements make it easier for you to use databases with R. The query examples below demonstrate some of the capabilities of these R packages.\n\nDBI. The DBI specification has gone through many recent improvements. When working with databases, you should always use packages that are DBI-compliant.\ndplyr & dbplyr. The dplyr package now has a generalized SQL backend for talking to databases, and the new dbplyr package translates R code into database-specific variants. As of this writing, SQL variants are supported for the following databases: Oracle, Microsoft SQL Server, PostgreSQL, Amazon Redshift, Apache Hive, and Apache Impala. More will follow over time.\nodbc. The odbc R package provides a standard way for you to connect to any database as long as you have an ODBC driver installed. The odbc R package is DBI-compliant, and is recommended for ODBC connections.\n\nRStudio also made recent improvements to its products so they work better with databases.\n\nRStudio IDE (v1.1 and newer). With the latest versions of the RStudio IDE, you can connect to, explore, and view data in a variety of databases. The IDE has a wizard for setting up new connections, and a tab for exploring established connections. These new features are extensible and will work with any R package that has a connections contract.\nRStudio Professional Drivers. If you are using RStudio Desktop Pro or other RStudio professional products, you can download RStudio Professional Drivers for no additional cost on the same machine where these products are installed. The examples below use the Oracle ODBC driver. If you are using open-source tools, you can bring your own driver or use community packages – many open-source drivers and community packages exist for connecting to a variety of databases.\n\nUsing databases with R is a broad subject and there is more work to be done. An earlier blog post discussed our vision.\n\n\nExample: Query bank data in an Oracle database\nIn this example, we will query bank data in an Oracle database. We connect to the database by using the DBI and odbc packages. This specific connection requires a database driver and a data source name (DSN) that have both been configured by the system administrator. Your connection might use another method.\nlibrary(DBI)\nlibrary(dplyr)\nlibrary(dbplyr)\nlibrary(odbc)\ncon <- dbConnect(odbc::odbc(), \"Oracle DB\")\n\n\n1. Query using DBI\nYou can query your data with DBI by using the dbGetQuery() function. Simply paste your SQL code into the R function as a quoted string. This method is sometimes referred to as pass through SQL code, and is probably the simplest way to query your data. Care should be used to escape your quotes as needed. For example, 'yes' is written as \\'yes\\'.\ndbGetQuery(con,'\n  select \"month_idx\", \"year\", \"month\",\n  sum(case when \"term_deposit\" = \\'yes\\' then 1.0 else 0.0 end) as subscribe,\n  count(*) as total\n  from \"bank\"\n  group by \"month_idx\", \"year\", \"month\"\n')\n\n\n2. Query using dplyr syntax\nYou can write your code in dplyr syntax, and dplyr will translate your code into SQL. There are several benefits to writing queries in dplyr syntax: you can keep the same consistent language both for R objects and database tables, no knowledge of SQL or the specific SQL variant is required, and you can take advantage of the fact that dplyr uses lazy evaluation. dplyr syntax is easy to read, but you can always inspect the SQL translation with the show_query() function.\nq1 <- tbl(con, \"bank\") %>%\n  group_by(month_idx, year, month) %>%\n  summarise(\n    subscribe = sum(ifelse(term_deposit == \"yes\", 1, 0)),\n    total = n())\nshow_query(q1)\n\n<SQL>\nSELECT \"month_idx\", \"year\", \"month\", SUM(CASE WHEN (\"term_deposit\" = 'yes') THEN (1.0) ELSE (0.0) END) AS \"subscribe\", COUNT(*) AS \"total\"\nFROM (\"bank\") \nGROUP BY \"month_idx\", \"year\", \"month\"\n\n\n3. Query using an R Notebooks\nDid you know that you can run SQL code in an R Notebook code chunk? To use SQL, open an R Notebook in the RStudio IDE under the File > New File menu. Start a new code chunk with {sql}, and specify your connection with the connection=con code chunk option. If you want to send the query output to an R dataframe, use output.var = \"mydataframe\" in the code chunk options. When you specify output.var, you will be able to use the output in subsequent R code chunks. In this example, we use the output in ggplot2.\n```{sql, connection=con, output.var = \"mydataframe\"}\nSELECT \"month_idx\", \"year\", \"month\", SUM(CASE WHEN (\"term_deposit\" = 'yes') THEN (1.0) ELSE (0.0) END) AS \"subscribe\",\nCOUNT(*) AS \"total\"\nFROM (\"bank\") \nGROUP BY \"month_idx\", \"year\", \"month\"\n```\n\n```{r}\nlibrary(ggplot2)\nggplot(mydataframe, aes(total, subscribe, color = year)) +\n  geom_point() +\n  xlab(\"Total contacts\") +\n  ylab(\"Term Deposit Subscriptions\") +\n  ggtitle(\"Contact volume\")\n```\n\nThe benefits to using SQL in a code chunk are that you can paste your SQL code without any modification. For example, you do not have to escape quotes. If you are using the proverbial spaghetti code that is hundreds of lines long, then a SQL code chunk might be a good option. Another benefit is that the SQL code in a code chunk is highlighted, making it very easy to read. For more information on SQL engines, see this page on knitr language engines.\n\n\nSummary\nThere is no single best way to query data with R. You have many methods to chose from, and each has its advantages. Here are some of the advantages using the methods described in this article.\n\n\n\n\n\n\n\nMethod\nAdvantages\n\n\n\n\n\nDBI::dbGetQuery\n\n\nFewer dependencies required\n\n\n\n\ndplyr syntax\n\n\nUse the same syntax for R and database objects\nNo knowledge of SQL required\nCode is standard across SQL variants\nLazy evaluation\n\n\n\n\nR Notebook SQL engine\n\n\nCopy and paste SQL – no formatting required\nSQL syntax is highlighted\n\n\n\n\nYou can download the R Notebook for these examples here."
  },
  {
    "href": "getting-started/connect-to-database.html",
    "title": "Connect to a Database",
    "section": "",
    "text": "This article is geared toward those who need to connect to an existing database using an ODBC driver. To review other options, such as using a database R package or JDBC, please refer to the Selecting a database interface page. If the intent is to learn with a local and small database, refer to the example in the dplyr page.\n\nInstall the DBI and odbc package:\ninstall.packages(\"DBI\")\ninstall.packages(\"odbc\")\nVerify that odbc recognizes the installed drivers using odbcListDrivers(). Here is an example result:\nlibrary(odbc)\nsort(unique(odbcListDrivers()[[1]]))\n[1] \"Devart ODBC Driver for PostgreSQL\"    \n[2] \"MapR Drill ODBC Driver\"               \n[3] \"ODBC Driver 13 for SQL Server\"        \n[4] \"Simba Athena ODBC Driver\"             \n[5] \"Simba ODBC Driver for Google BigQuery\"\n[6] \"SQL Server\"                           \nDetermine if a DSN is going to be used to connect to the database. This would be typically something that the Database Administrator, or the technical owner of the database, would setup and provide the R developer a name (known as an alias). Use dbConnect() to open a database connection in this manner:\ncon <- dbConnect(odbc(), \"DSN name\")\nIf no DSN is available, then the connection needs to usually pass the server address, database name, and the credentials to be used. There may be other settings unique to a given database that will also need to be passed. In the following example, the Trusted_Connection setting is unique to a MS SQL connection:\ncon <- dbConnect(odbc(), \n                 Driver = \"SQL Server\", \n                 Server = \"localhost\\\\SQLEXPRESS\", \n                 Database = \"datawarehouse\", \n                 Trusted_Connection = \"True\")\nFor information specific to how to connect to a specific database vendor, visit the Databases page for a list of the database types we have information available for."
  },
  {
    "href": "index.html",
    "title": "Databases using R",
    "section": "",
    "text": "Databases using R\nRStudio makes it as easy to work with databases in R. This work focuses on three key areas:\n\n\n\n\n1. RStudio products\n\nThe new RStudio Connections Pane makes it possible to easily connect to a variety of data sources, and explore the objects and data inside the connection\nTo RStudio commercial customers, we offer RStudio Professional ODBC Drivers, these are data connectors that help you connect to some of the most popular databases.\n\n\n\n2. R packages\nBuild and/or document how to use packages such as: dplyr, DBI, odbc, keyring and pool\n\n\n3. Education\nThis website is the main channel to provide support in this area. RStudio is also working through other delivery channels, such as upcoming webinars and in-person training during our RStudio conferences.\n\nRead more →"
  },
  {
    "href": "best-practices/select-interface.html#dbi-support-is-key",
    "title": "Selecting a database interface",
    "section": "DBI support is key",
    "text": "Connecting to a database with a method that supports the DBI package provides the following advantages:\n\nA consistent set of functions that work across all connections\nMakes using dplyr as a front-end possible\n\nMostly based on how difficult is to setup, the order of preference of connection methods is:\n\nNative database driver implemented in database package (e.g. RPostgresSQL). To see the list of known database packages check out the Databases page.\nODBC as implemented in odbc package\nJDBC as implemented in RJDBC package\n\nIf there is a need to connect to several databases in one or several R projects, then it would be preferable to use ODBC for all connections because it increases consistency.\nAdditionally, using the odbc package inside the RStudio IDE will enable the use of the Connections Pane"
  },
  {
    "href": "best-practices/select-interface.html#faq",
    "title": "Selecting a database interface",
    "section": "FAQ",
    "text": "Do I need to replace all of my existing JDBC connections, in favor of ODBC connections?\nNot necessarily, if there are existing R jobs that successfully run on a regular basis then there’s no need to introduce risk, in other words if it ain’t broke, don’t fix it.\nFor new projects, if using ODBC is still not a viable option, then using an R package that implements a DBI back-end for the JDBC connection will be advisable, like RJDBC. There are other packages that implement both a DBI back-end for a JDBC connection and a dplyr translation, such as implyr for Impala connections.\n\n\nCurrently, I use RODBC, should I consider switching to odbc?\nFor new projects, yes. Switching will enable dplyr interactivity, and the Connections Pane inside the RStudio IDE (see the previous section for more info).\nExisting projects may need to be individually vetted to see if the possible increase in performance that the odbc package may provide is worth making coding changes."
  },
  {
    "href": "best-practices/managing-credentials.html#integrated-security-with-dsn",
    "title": "Securing Credentials",
    "section": "Integrated security with DSN",
    "text": "The very best solution consists of two pieces that are usually out of the hands of the analyst:\n\nThere is integrated security between the terminal and the database, usually via Kerberos.\nAn ODBC connection has been pre-configured in the Desktop or Server by someone with sufficient access rights. The ODBC connection will have a unique Data Source Name, or DSN.\n\ncon <- DBI::dbConnect(odbc::odbc(), \"My DSN Name\")\nThis is considered the best option, because no information about the database, other than an internal alias, is available in plain text code."
  },
  {
    "href": "best-practices/managing-credentials.html#integrated-security-without-dsn",
    "title": "Securing Credentials",
    "section": "Integrated security without DSN",
    "text": "In cases where the analyst is not able to setup a DSN, or a quick prototype is needed, it is possible to pass the necessary connection settings as arguments to the DBI::dbConnect command. With integrated security, there are three arguments that are typically required: the driver name, the network path to the server (or cluster), and the database name. Other arguments may be required depending on the type of database; for instance, in the example below, the port number is required for a connection with an Apache Impala database.\ncon <- DBI::dbConnect(odbc::odbc(),\n                   Driver = \"impala\", \n                   Host = \"hadoop-cluster\",\n                   Schema = \"default\",\n                   Port   = 21050)"
  },
  {
    "href": "best-practices/managing-credentials.html#encrypt-credentials-with-keyring",
    "title": "Securing Credentials",
    "section": "Encrypt credentials with keyring",
    "text": "The keyring package uses the operating system’s credential storage. It currently supports:\n\nKeychain on MacOS\nCredential Store on Windows\nthe Secret Service API on Linux\n\nYou can now install from CRAN ::: {.cell}\ninstall.packages(\"keyring\")\n:::\nor the dev version from github ::: {.cell}\ndevtools::install_github(\"r-lib/keyring\")\n:::\n\nKeyrings and keys\nInside the credential store, there are keyrings that contain multiple keys. A keyring is secured with a single password, which then grants access to to all of its keys. In our case, we will store a single database connection credential per key. We will use the default keyring, which is automatically unlocked when a user signs in.\n\n\nOur first key\nA key has four main attributes:\n\nservice - The key’s unique identifier; we will use the database server’s path for our keys.\nkeyring - The key’s ‘parent’ keyring. If not specified, the default keyring is used.\nusername\npassword\n\nThe way keyring retrieves data is by passing the keyring name and a service name. If no keyring is passed, the default keyring is used. The keyring::key_set() function is used to create the key; a prompt will appear asking for the password that should be used for the key:\n\nkeyring::key_set(service = \"my-database\", \n                 username = \"myusername\")\n\n\n\nRetrieve credentials\nThe key_list() command is used to retrieve the username:\n\nkeyring::key_list(\"my-database\")\n\nTo extract the only the username, so as to pass it inside the connection string, use:\n\nkeyring::key_list(\"my-database\")[1,2]\n\nTo extract the password, use key_get():\n\nkeyring::key_get(\"my-database\", \"myusername\")\n\nThese functions can be used to supply the database credentials without storing them in plain text or an environment variable:\n\ncon <- dbConnect(odbc::odbc(), \n  Driver   = \"SQLServer\",\n  Server   = \"my-database\",\n  Port     = 1433,\n  Database = \"default\",\n  UID      = keyring::key_list(\"my-database\")[1,2],\n  PWD      = keyring::key_get(\"my-database\", \"myusername\"))\n\nThe default keyring is unlocked anytime the user is signed in. If a new keyring is created and used, the Operating System will prompt the user for the keyring password when the code executes."
  },
  {
    "href": "best-practices/managing-credentials.html#stored-in-a-file-with-config",
    "title": "Securing Credentials",
    "section": "Stored in a file with config",
    "text": "The config package is meant to make it easier to deploy content, but we also use it to keep the credentials outside of the R script by saving them in the config.yml file. In fact, all of the connection arguments can be saved in the yml file.\n  install.packages(\"config\")\nHere is an example yml file:\ndefault:\n  datawarehouse:\n    driver: 'Postgres' \n    server: 'mydb-test.company.com'\n    uid: 'local-account'\n    pwd: 'my-password'  \n    port: 5432\n    database: 'regional-sales'\n \nThis is how the connection arguments would be called inside the R script:\n\ndw <- config::get(\"datawarehouse\")\n\ncon <- DBI::dbConnect(odbc::odbc(),\n   Driver = dw$driver,\n   Server = dw$server,\n   UID    = dw$uid,\n   PWD    = dw$pwd,\n   Port   = dw$port,\n   Database = dw$database\n)"
  },
  {
    "href": "best-practices/managing-credentials.html#use-environment-variables",
    "title": "Securing Credentials",
    "section": "Use Environment variables",
    "text": "The .Renviron file can be used to store the credentials, which can then be retrieved with Sys.getenv(). Here are the steps:\n\nCreate a new file defining the credentials:\n\n\n    userid = \"username\"\n    pwd = \"password\"\n\n\nSave it in your home directory with the file name .Renviron. If you are asked whether you want to save a file whose name begins with a dot, say YES.\nNote that by default, dot files are usually hidden. However, within RStudio, the file browser will make .Renviron visible and therefore easy to edit in the future.\nRestart R. .Renviron is processed only at the start of an R session.\nRetrieve the credentials using Sys.getenv() while opening the connection: ::: {.cell}\n\n  con <- DBI::dbConnect(odbc::odbc(),\n    Driver = \"impala\", \n    Host   = \"database.rstudio.com\",\n    UID    = Sys.getenv(\"userid\"),\n    PWD    = Sys.getenv(\"pwd\")\n  )\n:::"
  },
  {
    "href": "best-practices/managing-credentials.html#using-options",
    "title": "Securing Credentials",
    "section": "Using options()",
    "text": "You can record the user name and password as a global option in R. Use the options() command to set a custom option, and then use getOption() to retrieve it.\nThe following example code sets credentials. When trying this out, be sure to remove these lines from your published work:\n\n  options(database_userid = \"myuserid\")\n  options(database_password = \"mypassword\")\n\nThis is how the credentials can be called within the published work:\n\n  con <- DBI::dbConnect(odbc::odbc(),\n    Driver = \"impala\", \n    Host   = \"database.rstudio.com\",\n    UID    = getOption(\"database_userid\"),\n    PWD    = getOption(\"database_password\")\n  )"
  },
  {
    "href": "best-practices/managing-credentials.html#prompt-for-credentials",
    "title": "Securing Credentials",
    "section": "Prompt for Credentials",
    "text": "The RStudio IDE’s API can be used to prompt the user to enter the credentials in a popup box that masks what is typed:\n\n\n\n\n\n\ncon <- DBI::dbConnect(odbc::odbc(),\n  Driver = \"impala\", \n  Host   = \"database.rstudio.com\",\n  UID    = rstudioapi::askForPassword(\"Database user\"),\n  PWD    = rstudioapi::askForPassword(\"Database password\")\n)"
  },
  {
    "href": "best-practices/portable-code.html#deploying-with-dsns",
    "title": "Making scripts portable",
    "section": "Deploying with DSNs",
    "text": "One way to manage ODBC connections is through Data Source Names. DSNs define the details for a connection including the server IP address, the database name, and often access credentials. DSNs are typically defined by an administrator, and are accessible across the server. It also possible to have user-specific or project-specific DSNs.\nDeploying content that uses a DSN is easy. The R code would look like:\nlibrary(DBI)\nlibrary(odbc)\n\ncon <- dbConnect(odbc::odbc(), \"My DSN Name\")\nMy DSN Name would correspond to an entry in an odbc.ini file. For example, the DSN name “PostgresSQL2” would be used to reference the corresponding entry in this hypothetical odic.ini file:\n[PostgreSQL2]\nDriver = /etc/lib/driver/libpsql.so\nDatabase = mydb.company.com\nPort = 5432\nUID = service\nPWD = serviceAcc0unt\n\n[SalesWarehouse]\nDriver = /etc/lib/drivers/oraclesqlib.so\n...\nThe key is to ensure that the DSN in the development environment and the DSN in the deployment environment refer to the same database. It is possible that the two DSNs will use different credentials or even different drivers, but the queries submitted to the connection should work in both environments.\nBe careful: it is easy for DSNs to differ between the development and deployment environment. In some cases, the differences will lead to an error. For example, if the DSN used locally does not exist on the server, the deployment will likely fail. In other cases, it is possible that the DSNs will be similar enough for deployment to succeed, but different enough for other parts of the code to fail.\nWhen in doubt, be sure to ask your administrator to compare the odbc.ini and odbcinst.ini files used locally and on the deployed server."
  },
  {
    "href": "best-practices/portable-code.html#deploying-with-the-config-package",
    "title": "Making scripts portable",
    "section": "Deploying with the config Package",
    "text": "An alternative to relying on DSNs is to use the config package. The config package allows the connection code in R to reference an external file that defines values based on the environment. This process makes it easy to specify values to use for a connection locally and values to use after deployment.\nFor example:\nR code:\nlibrary(DBI)\nlibrary(odbc)\nlibrary(config)\n\ndw <- get(\"datawarehouse\")\n\ncon <- dbConnect(\n   Driver = dw$driver,\n   Server = dw$server,\n   UID    = dw$uid,\n   PWD    = dw$pwd,\n   Port   = dw$port,\n   Database = dw$database\n)\nconfig.yml:\ndefault:\n  datawarehouse:\n    driver: 'Postgres' \n    server: 'mydb-test.company.com'\n    uid: 'local-account'\n    pwd: 'my-password'  // not recommended, see alternatives below\n    port: 5432\n    database: 'regional-sales-sample'\n    \nrsconnect:\n  datawarehouse:\n    driver: 'PostgresPro'\n    server: 'mydb-prod.company.com'\n    uid: 'service-account'\n    pwd: 'service-password' // not recommended, see alternatives below\n    port: 5432\n    database: 'regional-sales-full'\nThe config package determines the active configuration by looking at the R_CONFIG_ACTIVE environment variable. By default, RStudio Connect sets R_CONFIG_ACTIVE to the value rsconnect. In the config file above, the default datawarehouse values would be used locally and the datawarehouse values defined in the rsconnect section would be used on RStudio Connect. Administrators can optionally customize the name of the active configuration used in Connect."
  },
  {
    "href": "best-practices/portable-code.html#deploying-with-the-config-package-and-credentials",
    "title": "Making scripts portable",
    "section": "Deploying with the config Package and Credentials",
    "text": "In the previous example, the credentials used locally and after deployment were stored in plain text in the configuration file. There are many ways to avoid plain text storage. It is common for the method used to access credentials locally to vary from the method used for deployed content. For example, the following config uses keyring to access credentials locally, but accesses an environment variable on the deployed server.\nconfig.yml:\ndefault:\n  datawarehouse:\n    driver: 'Postgres' \n    server: 'mydb-test.company.com'\n    uid: !expr keyring::key_get(\"db-credentials\")[1,2]'\n    pwd: !expr keyring::key_get(\"db-credentials\")[1,2]'\n    port: 5432\n    database: 'regional-sales-sample'\n    \nrsconnect:\n  datawarehouse:\n    driver: 'PostgresPro'\n    server: 'mydb-prod.company.com'\n    uid: !expr Sys.getenv(\"DBUSER\")\n    pwd: !expr Sys.getenv(\"DBPWD\")\n    port: 5432\n    database: 'regional-sales-full'"
  },
  {
    "href": "best-practices/portable-code.html#deploying-with-different-dsns",
    "title": "Making scripts portable",
    "section": "Deploying with Different DSNs",
    "text": "The config package provides a very flexible way to specify connections. It is even possible to use the config package with DSNs! For example, if the local DSN entry was named “DatawarehouseTest” and the DSN on the production server was named “DatawarehouseProd”:\nR code:\nlibrary(DBI)\nlibrary(odbc)\nlibrary(config)\n\ndsn <- get(\"datawarehouse\")\n\ncon <- dbConnect(odbc::odbc(), dsn)\nconfig.yml\ndefault:\n  datawarehouse: 'DatawarehouseTest'\n\nrsconnect:\n  datawarehouse: 'DatawarehouseProd'\nIn all cases, it is important that the R code (whether a R Markdown document or a Shiny application) be able to run successfully in both the development and deployment environment without changes. It is not possible to change hard-coded values prior to deployment."
  },
  {
    "href": "best-practices/dashboards.html#design-principles",
    "title": "Enterprise-ready dashboards",
    "section": "Design principles",
    "text": "A few principles to keep in mind when developing an enterprise level dashboard:\n\nPush as much of the calculations of the dashboard back to the database - The time it takes for a dashboard to load, and respond, will become the most important aspect of its design. For dashboards, the expected time to load and response is a few seconds.\nGive the end-user with “train of thought” paths - These paths are commonly provided by way of a drill down from within the dashboard. These paths allows the end-user to quickly answer questions they currently have.\nMake data driven inputs - It is easy to “hard code” the values that will be available in an input, such as a drop down. This may become a problem later on if the possible values change over time. An ancillary principle is to always use a look up table to obtain the list of values, when available. It is not ideal to obtain a list of possible values by grouping and summarizing a large column.\nSecure the database credentials - Most often, a service account is made available for reports and dashboards that have access to a database. In that case, it is important to know how to properly safeguard the credentials of the service account."
  },
  {
    "href": "best-practices/dashboards.html#example",
    "title": "Enterprise-ready dashboards",
    "section": "Example",
    "text": "Click to see the live dashboard\n\n\n\n\n\nA working example of the dashboard that will be used as the base of this article is available on GitHub. The project repository contains the code and all the supporting files:\n\nVisit the repository: https://github.com/sol-eng/db-dashboard\nDownload a zipped file with all of the files from the repository: https://github.com/sol-eng/db-dashboard/archive/master.zip\n\nA live version of the app can be found in shinyapps.io: DB Dashboard"
  },
  {
    "href": "best-practices/dashboards.html#use-shinydashboard",
    "title": "Enterprise-ready dashboards",
    "section": "Use shinydashboard",
    "text": "The shinydashboard package has three important advantages:\n\nProvides an out-of-the-box framework to create dashboards in Shiny. This saves a lot of time, because the developer does not have to create the dashboard features manually using “base” Shiny.\nHas a dashboard-firendly tag structure. This allows the developer to get started quickly. Inside the dashboardPage()tag, the dashboardHeader(), dashboardSidebar() and dashboardBody() can be added to easily lay out a new dashboard.\nIt is mobile-ready. Without any additional code, the dashboard layout will adapt to a smaller screen automatically.\n\nAnother option for creating dashboard with R is flexdashboard. It will support all but one of the features discussed in this article: dynamic tabs, which are the basis of the technique used in the example to drive the drill throughs.\n\nQuick example\nIf you are new to shinydashboard, please feel free to copy and paste the following code to see a very simple dashboard in your environment:\nlibrary(shinydashboard)\nlibrary(shiny)\nui <- dashboardPage(\n  dashboardHeader(title = \"Quick Example\"),\n  dashboardSidebar(textInput(\"text\", \"Text\")),\n  dashboardBody(\n    valueBox(100, \"Basic example\"),\n    tableOutput(\"mtcars\")\n  )\n)\nserver <- function(input, output) {\n  output$mtcars <- renderTable(head(mtcars))\n}\nshinyApp(ui, server)"
  },
  {
    "href": "best-practices/dashboards.html#connection-strings-and-credentials",
    "title": "Enterprise-ready dashboards",
    "section": "Connection strings and credentials",
    "text": "In many cases, the app is developed against one database, and run in production against a different database. This can present a challenge if the connection string is “hard coded”.\nThis site provides a couple of articles to help with addressing credentials and portability of code:\n\nSecuring Deployed Content\nMaking Scripts Portable"
  },
  {
    "href": "best-practices/dashboards.html#populate-shiny-inputs-using-purrr",
    "title": "Enterprise-ready dashboards",
    "section": "Populate Shiny inputs using purrr",
    "text": "The usual preference is for the values displayed in a user input, such as drop down, to be “human readable”. The actual value of the selection should be a unique identifier so that dependent queries return the correct information.\nThis section examines two cases and ways to format the list of options to be in a format that Shiny can use.\n\nDrop down populated from database\nIdeally, a look up table is available in the database so that the query is simple to execute.\nTo separate the keys from the values, the map() function in the purrr package can be used. In the example below, all of the records in the airlines table are collected, and a list of names is created, map() is then used to insert the carrier codes into each name node.\nairline_list <- tbl(con, \"airlines\") %>%\n  collect  %>%\n  split(.$name) %>%    # Field that will be used for the labels\n  map(~.$carrier)      # Field that will be used for keys\nThe selectInput() drop-down menu is able to read the resulting airline_list list variable.\n  dashboardSidebar(\n    selectInput(\n      inputId = \"airline\",\n      label = \"Airline:\",\n      choices = airline_list,\n      selected = \"DL\",\n      selectize = FALSE\n    )\n\n\nList populated from a vector\nThere are times when the possible values are static, and small enough, so that they all fit in a vector.\nA common example is the month name. A given table store the month number, and that number is what needs to be used as the filter value The values presented to the end-user will be the month name, but when a selection is made, the month number is what will be passed to Shiny.\nFor that, a function called set_names() can be used to add the caption that will be displayed in the input in a way that is “Shiny friendly”\nmonth_list <- as.list(1:12) %>%\n  set_names(month.name)\n  \nmonth_list$`All Year` <- 99  \nThe selectInput() list menu is able to read the resulting month_list list variable.\n  selectInput(\n    inputId = \"month\",\n    label = \"Month:\",\n    choices = month_list,\n    selected = 99,\n    size = 13,\n    selectize = FALSE\n  )"
  },
  {
    "href": "best-practices/dashboards.html#create-a-base-query-using-dplyr",
    "title": "Enterprise-ready dashboards",
    "section": "Create a base query using dplyr",
    "text": "In most cases, all of the plots and tables in a dashboard share a common base query. For example, they will all show the same month’s data. Using dplyr to build the base query has the following advantages:\n\nSimplifies the code because it prevents the repetition of filters and joins.\ndplyr “laziness” allows for the base query to be built with out it being executed until it is used to get the data for a given plot or table.\nAbstracts the translation of the SQL syntax. The dashboard will work with no, or minimal, changes if the database vendor changes.\nThe modular nature of this approach allows to just add a few simple, and easy to understand, dplyr steps to get the slice or aggregation of the data needed to be displayed on the plot or table.\n\nBecause the base query will more likely have to be assembled based on the current input selection, then a Shiny reactive() function is necessary to be used instead of a regular function(). This is because the input$... variables can only be evaluated inside a Shiny reactive function.\n  base_flights <- reactive({\n    res <- flights %>%\n      filter(carrier == input$airline) %>%\n      left_join(airlines, by = \"carrier\") %>%\n      rename(airline = name) %>%\n      left_join(airports, by = c(\"origin\" = \"faa\")) %>%\n      rename(origin_name = name) %>%\n      select(-lat, -lon, -alt, -tz, -dst) %>%\n      left_join(airports, by = c(\"dest\" = \"faa\")) %>%\n      rename(dest_name = name)\n    if (input$month != 99) res <- filter(res, month == input$month)\n    res\n  })\nThen, the Shiny output function starts with the base query (base_flights), and finishing dplyr steps, in the form of verbs, are appended, and piped directly to the plotting or display function. It is important to note that before sending the resulting data set to Shiny, either a collect() or pull() function needs to be used.\n  output$per_day <- renderValueBox({\n    base_flights() %>%      #------ Base query \n      group_by(day, month) %>%  #-- Finishing steps   \n      tally() %>%                  \n      summarise(avg = mean(n)) %>%\n      pull() %>%\n      round() %>%\n      prettyNum(big.mark = \",\") %>% \n      valueBox(             # -- Pipe right into a Value Box\n        subtitle = \"Average Flights per day\",\n        color = \"blue\"\n      )\n  })"
  },
  {
    "href": "best-practices/dashboards.html#using-r2d3-for-interactivity-and-drill-down",
    "title": "Enterprise-ready dashboards",
    "section": "Using r2d3 for interactivity and drill-down",
    "text": "A “drill-down” is a great way to provide the end-user with “train of thought” paths.\nIn a Shiny app or dashboard, there’s the R object that contains the plot or table needs a way to pass to Shiny the value what was that was clicked on. The best way to do this, is by using Shiny’s JavaScript inside a given plot. This activates a reactive function inside the app.\nThe visualization packages called htmlwidgets are widely used. They are a set of packages are wrappers on top of D3/JavaScript plots. There may be times when the available htmlwidgets package falls short, either by not integrating with Shiny, or by not providing the exact visualization that is needed for the dashboard.\nThis article, the package r2d3 will be used. This package allows us to custom build D3 visualizations from the ground up, for maximum flexibility and best integration with Shiny. A more in-depth article on how to integrate Shiny with r2d3 is available here: Using r2d3 with Shiny.\n\nTwo ready-to-use r2d3 plots\nThe example dashboard used in this article contains two D3 scripts that are “Shiny-ready”. One is a column plot and the other a bar plot. They have been developed in a way that you can easily copy the entire script and use it in your own dashboard.\n\ncol_plot.js - Requires a data.frame or tibble with the following names and type of data:\n\nx - Expects the category’s value. For example, if it represents a month, then it would contain the month’s number.\ny - Expects the value of the height of the column.\nlabel - Expects the category’s caption. It is what will be displayed to the end-user. For example, if it represents a month, then it would contain the month’s name.\n\nbar_plot.js - - Requires a data.frame or tibble with the following names and type of data:\n\nx - Expects the value of the width of the bar.\ny - Expects the category’s value. For example, if it represents a month, then it would contain the month’s number.\nlabel - Expects the category’s caption. It is what will be displayed to the end-user. For example, if it represents a month, then it would contain the month’s name.\n\n\nThanks to r2d3, the plots can easily be rendered. This code snippet shows how simple is to combine the technique of using a base query, and then pipe the finishing transformations directly into the r2d3() function.\n  output$top_airports <- renderD3({\n    # The following code runs inside the database\n    base_flights() %>%\n      group_by(dest, dest_name) %>%\n      tally() %>%\n      collect() %>%\n      arrange(desc(n)) %>%\n      head(10) %>%\n      arrange(dest_name) %>%\n      mutate(dest_name = str_sub(dest_name, 1, 30)) %>%\n      rename(\n        x = dest,              # Make sure to rename the \n        y = n,                 # variables to what the \n        label = dest_name      #  D3 script expects\n      ) %>%\n      r2d3(\"bar_plot.js\")\n  })"
  },
  {
    "href": "best-practices/dashboards.html#handling-a-click-event-from-the-plot",
    "title": "Enterprise-ready dashboards",
    "section": "Handling a click event from the plot",
    "text": "The ideal outcome of a click event is that it activates a Shiny input. This allows the app to execute a reactive function when the click, or any other event recognized by the plot, is triggered.\nThe D3 plots, available in the example’s GitHub repository, already contain the necessary Shiny JS code to trigger a reactive function when clicked on:\n\ncol_plot.js - Creates a input$col_clicked inside the Shiny app.\nbar_plot.js - Creates a input$bar_clicked inside the Shiny app.\n\nInside the app, include an observeEvent() function that will capture the value returned by the D3 plot:\n  observeEvent(input$bar_clicked, {\n  # ----- Function's code --------\n  })\nTroubleshooting tip - If the nothing happens when a bar is clicked on, please confirm that the installed shiny package version is 1.1.0 or above."
  },
  {
    "href": "best-practices/dashboards.html#create-the-drill-down-report",
    "title": "Enterprise-ready dashboards",
    "section": "Create the drill-down report",
    "text": "Using appendTab() to create the drill-down report\nThe plan is to display a new drill-down report every time the end user clicks on a bar. To prevent pulling the same data unnecessarily, the code will be “smart” enough to simply switch the focus to an existing tab if the same bar has been clicked on before. This switch also prevent unnecessary trips to the database.\nThe new, and really cool, appendTab() function is used to dynamically create a new Shiny tab with a data table from the DT package that contains the first 100 rows of the selection. A simple vector, called tab_list, is used to track all existing detail tabs. The updateTabsetPanel() function is used to switch to the newly or previously created tab.\nThe observeEvent() function is the one that “catches” the event executed by the D3 code. It monitors the bar_clicked Shiny input.\n  observeEvent(input$bar_clicked, {\n    airport <- input$bar_clicked\n    month <- input$month\n    tab_title <- paste(\n      input$airline, \"-\", airport,\n      if (month != 99) {\n        paste(\"-\", month.name[as.integer(month)])\n      }\n    )\n    if (!(tab_title %in% tab_list)) {\n      appendTab(\n        inputId = \"tabs\",\n        tabPanel(\n          tab_title,\n          DT::renderDataTable(\n            # This function return a data.frame with\n            # the top 100 records of that airport\n            get_details(airport = airport) \n          )\n        )\n      )\n\n      tab_list <<- c(tab_list, tab_title)\n    }\n    updateTabsetPanel(session, \"tabs\", selected = tab_title)\n  })\n\n\nRemove all tabs using removeTab() and purrr\nCreating new tabs dynamically can clutter the dashboard. So a simple actionLink() button can be added to the dashboardSidebar() in order to remove all tabs except the main dashboard tab.\n# This code runs in ui\n  dashboardSidebar(\n       actionLink(\"remove\", \"Remove detail tabs\"))\nThe observeEvent() function is used once more to catch when the link is clicked. The walk() command from purrr is then used to iterate through each tab title in the tab_list vector and proceeds to execute the Shiny removeTab() command for each name. After that, the tab list variable is reset. Because of environment scoping, make sure to use double less than ( <<- ) when resetting the variable, so it knows to reset the variable defined outside of the observeEvent() function.\n# This code runs in server\n  observeEvent(input$remove,{\n    # Use purrr's walk command to cycle through each\n    # panel tabs and remove them\n    tab_list %>%\n      walk(~removeTab(\"tabs\", .x))\n    tab_list <<- NULL\n  })"
  },
  {
    "href": "best-practices/dashboards.html#full-example",
    "title": "Enterprise-ready dashboards",
    "section": "Full example",
    "text": "There are two versions of the app available in the GitHub repository:\n\nlocal_app.R - Example that works without a database connection.\ndb_app.R - Full example, it shows how it connects to a database."
  },
  {
    "href": "best-practices/drivers.html#using-rstudio-professional-drivers",
    "title": "Setting up ODBC Drivers",
    "section": "Using RStudio Professional Drivers",
    "text": "When working with databases on RStudio Desktop Pro and other RStudio professional products, it is strongly recommended to use the RStudio Professional Drivers. Not only these come with full support, but also they simplify the installation and configuration process is most cases, not requiring many of the steps detailed below.\n\n\n\nDiagram of Using RStudio’s Professional Drivers"
  },
  {
    "href": "best-practices/drivers.html#using-other-drivers",
    "title": "Setting up ODBC Drivers",
    "section": "Using other drivers",
    "text": "For Linux and MacOS, ODBC drivers should be compiled against unixODBC. Drivers compiled against iODBC may also work, but are not fully supported.\nAfter installation of the driver manager and driver, you will have to register the driver in a odbcinst.ini file for it to appear in odbc::odbcListDrivers()."
  },
  {
    "href": "best-practices/drivers.html#microsoft-windows",
    "title": "Setting up ODBC Drivers",
    "section": "Microsoft Windows",
    "text": "Database Drivers\nWindows is bundled with ODBC libraries; however, drivers for each database need to be installed separately. Windows ODBC drivers typically include an installer that must be run to install the drivers in the proper locations.\n\n\nAdministration\nThe ODBC Data Source Administrator application is used to manage ODBC data sources on Windows."
  },
  {
    "href": "best-practices/drivers.html#apple-macos",
    "title": "Setting up ODBC Drivers",
    "section": "Apple MacOS",
    "text": "Installation\n\nInstall homebrew to install database drivers easily on MacOS\nInstall UnixODBC, which is required for all databases\n\n  # Install the unixODBC library\n  brew install unixodbc\n\nInstall common DB drivers (optional)\n\n  # SQL Server ODBC Drivers (Free TDS)\n  brew install freetds --with-unixodbc\n  \n  # PostgreSQL ODBC ODBC Drivers\n  brew install psqlodbc\n  \n  # MySQL ODBC Drivers (and database)\n  brew install mysql\n  \n  # SQLite ODBC Drivers\n  brew install sqliteodbc\n\n\nSetting up database connections\nSee the section with the same name in the Linux section."
  },
  {
    "href": "best-practices/drivers.html#linux-debian-ubuntu",
    "title": "Setting up ODBC Drivers",
    "section": "Linux Debian / Ubuntu",
    "text": "Installation\nThe apt-get command can be used to install database drivers easily on Linux distributions that support it, such as Debian and Ubuntu.\n\nInstall UnixODBC, which is required for all databases\n\n  # Install the unixODBC library\n  apt-get install unixodbc unixodbc-dev --install-suggests\n\nInstall common DB drivers (optional)\n\n  # SQL Server ODBC Drivers (Free TDS)\n  apt-get install tdsodbc\n  \n  # PostgreSQL ODBC ODBC Drivers\n  apt-get install odbc-postgresql\n  \n  # MySQL ODBC Drivers\n  apt-get install libmyodbc\n  \n  # SQLite ODBC Drivers\n  apt-get install libsqliteodbc\n\n\nSetting up database connections\nOn MacOS and Linux, there are two separate text files that need to be edited. UnixODBC includes a command-line executable called odbcinst, which can be used to query and modify the DSN files. However, these are plain text files you can also edit by hand if desired.\nThere are two different files used to set up the DSN information:\n\nodbcinst.ini defines driver options\nodbc.ini defines connection options\n\n\n\nodbcinst.ini\nThis file contains the driver information, particularly the name of the driver library. Multiple drivers can be specified in the same file.\n[PostgreSQL Driver]\nDriver          = /usr/local/lib/psqlodbcw.so\n\n[SQLite Driver]\nDriver          = /usr/local/lib/libsqlite3odbc.dylib\n\n\nodbc.ini\nThis file contains the connection information, particularly the username, password, database and host information. The Driver line corresponds to the driver defined in odbcinst.ini.\n[PostgreSQL]\nDriver              = PostgreSQL Driver\nDatabase            = test_db\nServername          = localhost\nUserName            = postgres\nPassword            = password\nPort                = 5432\n\n[SQLite]\nDriver          = SQLite Driver\nDatabase=/tmp/testing\nSee also: unixODBC without the GUI for more information and examples.\n\n\nLocation\nThe DSN configuration files can be defined globally for all users of the system, often at /etc/odbc.ini or /opt/local/etc/odbc.ini. The file location depends on what option was used when compiling unixODBC; odbcinst -j can be used to find the exact location. Alternatively, the ODBCSYSINI environment variable can be used to specify the location of the configuration files. Ex. ODBCSYSINI=~/ODBC\nA local DSN file can also be used with the files ~/.odbc.ini and ~/.odbcinst.ini."
  },
  {
    "href": "best-practices/drivers.html#connecting-to-a-database-in-r",
    "title": "Setting up ODBC Drivers",
    "section": "Connecting to a Database in R",
    "text": "Databases can be connected by specifying a connection string directly, or with DSN configuration files.\n# Install the latest odbc release from CRAN:\ninstall.packages(\"odbc\")\n\n# Or the the development version from GitHub:\n# install.packages(devtools)\ndevtools::install_github(\"rstats-db/odbc\")\n\nConnection Strings\nPass the connection parameters as arguments to the dbConnect() function.\ncon <- DBI::dbConnect(odbc::odbc(),\n  driver = \"PostgreSQL Driver\",\n  database = \"test_db\",\n  UID    = rstudioapi::askForPassword(\"Database user\"),\n  PWD    = rstudioapi::askForPassword(\"Database password\"),\n  host = \"localhost\",\n  port = 5432)\nFor database-specific settings, go to the Databases section in the menu and look for the page that matches the desired database type.\n\n\nDSN Configuration files\nODBC configuration files are another option to specify connection parameters; they allow you to use a Data Source Name (DSN) to make it easier to connect to a database.\n  con <- dbConnect(odbc::odbc(), \"PostgreSQL\")\nFor more information about how DSN can be used, check out the following articles in this site:\n\nSecuring Credentials\nMaking Scripts Portable\nSecuring Deployed Content"
  },
  {
    "href": "best-practices/visualization.html#bar-plot",
    "title": "Creating Visualizations",
    "section": "Bar plot",
    "text": "A Bar plot is intended to measure and compare categorical data. Passing the category to geom_bar() as x will automatically calculate the height of the bars based on the row count per category. Here is the code of a typical bar plot using ggplot2:\n\nggplot(data = flights) +\n  geom_bar(aes(x = origin), stat = \"count\")\n\n\nData transformation\nBecause dplyr is being used to compute the count per category inside the database, the discrete values are separated using group_by(), followed by tally() to obtain the row count per category. Lastly, collect() downloads the results into R:\n\ndf <- tbl(con, \"flights\") %>%\n  group_by(origin) %>%\n  tally() %>%\n  collect()\n\ndf\n\n\n\nPlotting results in R\nThe results of the Data Transformation step can now be used in ggplot2 to render the plot. This time, geom_col() is used instead of geom_bar() because the height of the bars have been pre-calculated by dplyr:\n\nggplot(data = df) +\n  geom_col(aes(x = origin, y = n)) \n\n\n\nTransform and plot\nThe plot can be created using a single piped line of code. This is particularly useful when performing exploratory data analysis because it is easy to add or remove filters, or to change the variable that is being analyzed.\n\ntbl(con, \"flights\") %>%\n  group_by(origin) %>%\n  tally() %>%\n  collect() %>%\n  ggplot() +\n    geom_col(aes(x = origin, y = n))"
  },
  {
    "href": "best-practices/visualization.html#histogram",
    "title": "Creating Visualizations",
    "section": "Histogram",
    "text": "The histogram is intended to visualize the distribution of the values of a continuous variable. It does this by grouping the values into bins with the same range of values. In essence, a histogram converts a continuous variable to a discrete variable by splitting and placing the variable’s values into multiple bins.\n\nCalculations\nThe following breakdown of the calculation needed to create a histogram is intended to highlight the complexity of moving its processing to the database.\nFor example, if a histogram with 20 bins is needed, and the variable has a minimum value of 1 and a maximum value of 101, then each bin needs to be 5.\n\n101 (Max value) - 1 (Min value) = 100\n100 / 20 (Number of bins) = 5\n\nThe first bin will have a range of 1 to 6, the second 7 to 12, etc.\nAfter that, the count of values that are inside each range needs to be determined. In this example, there may be two rows that have a value between 1 and 6 and five rows with values between 7 and 12.\nAny formula used to create a Histogram will need to calculate the bins, place the values inside the bins, and only call math functions supported by the database in use.\n\n\nUsing a helper function\nAn advantage of using dplyr to convert the continuous variable into a discrete variable is that one solution can be applied to multiple database types. This is possible if the resulting formula is made of basic functions that most SQL databases support and is expressed in R, so that dplyr can translate it into the proper SQL syntax.\nUnfortunately, the formula is rather long and mistakes can be made if used in multiple locations, because any corrections to the formula may not be propagated to all of the instances. To solve this, a helper function can be used.\nIn the following helper function, the var input is used to build the formula in an unevaluated R code format. When used inside dplyr, it will return the assembled formula which will then be evaluated as inside the verb command. Feel free to copy this function into your script or R Notebook.\nThe function has two other arguments:\n\nbins - this allows the number of bins to be customized. It defaults to 30\nbinwidth - this is used to specify the size of the bin. It overrides any value passed to the bins argument.\n\n\nlibrary(rlang)\ndb_bin <- function(var, bins = 30, binwidth = NULL) {\n  var <- enexpr(var)\n\n  range <- expr((max(!! var, na.rm = TRUE) - min(!! var, na.rm = TRUE)))\n\n  if (is.null(binwidth)) {\n    binwidth <- expr((!! range / !! bins))\n  } else {\n    bins <- expr(as.integer(!! range / !! binwidth))\n  }\n\n  # Made more sense to use floor() to determine the bin value than\n  # using the bin number or the max or mean, feel free to customize\n  bin_number <- expr(as.integer(floor((!! var - min(!! var, na.rm = TRUE)) / !! binwidth)))\n\n  # Value(s) that match max(x) will be rebased to bin -1, giving us the exact number of bins requested\n  expr(((!! binwidth) *\n    ifelse(!! bin_number == !! bins, !! bin_number - 1, !! bin_number)) + min(!! var, na.rm = TRUE))\n}\n\nNotice that the function returns a quosure containing the unevaluated R code that calculates the bins. To read more about how this kind of approach works, please refer to this article: Programming with dplyr.\nIt is important to note that the database in use needs to support the functions called in the formula, such as min() and max().\nHere is an example of the function’s output. Notice that a fictitious field called any_field is used, and no “missing field” error is generated. That is because the formula has not yet been evaluated.\n\ndb_bin(any_field)\n\nThis is an example of the function using binwidth. The resulting formula is a little different.\n\ndb_bin(any_field, binwidth = 300)\n\n\n\nData transformation\nThe data processing is very simple when using the helper function. The db_bin function is used inside group_by(). There are a couple of must-do’s to keep in mind:\n\nSpecify the name of the field that uses the db_bin() function - If a name is not specified, dplyr will use the long formula text as the default name of the field, which in most cases breaks the database’s field name length rules.\nPrefix !! to the db_bin() function - This triggers the processing, or evaluation, of the function, which returns the complex formula.\n\n\ndf <- tbl(con, \"flights\") %>%\n  group_by(x = !! db_bin(sched_dep_time, bins = 10)) %>%\n  tally() %>%\n  collect()\n\nhead(df)\n\n\n\nPlotting results in R\nBecause the bins have been pre-processed on and collected from the database, the results are easily plotted using geom_col(). The resulting bin values are x and the count per bin is y:\n\nggplot(data = df) +\n  geom_col(aes(x = x, y = n))\n\n\n\nTransform and plot\nJust like with the Bar plot, the entire process can be piped. Here is an example of using the binwidth argument instead of bins; additionally, the bin size is widened to 300-minute intervals:\n\ntbl(con, \"flights\") %>%\n  group_by(x = !! db_bin(sched_dep_time, binwidth = 300)) %>%\n  tally() %>%\n  collect() %>%\n  ggplot() +\n    geom_col(aes(x = x, y = n))"
  },
  {
    "href": "best-practices/visualization.html#raster-plot",
    "title": "Creating Visualizations",
    "section": "Raster Plot",
    "text": "To visualize two continuous variables, we typically resort to a Scatter plot. However, this may not be practical when visualizing millions or billions of dots representing the intersections of the two variables. A Raster plot may be a better option, because it concentrates the intersections into squares that are easier to parse visually.\nA Raster plot basically does the same as a Histogram. It takes two continuous variables and creates discrete 2-dimensional bins represented as squares in the plot. It then determines either the number of rows inside each square or processes some aggregation, like an average.\n\nData transformation\nThe same helper function used to create the Histogram can be used to create the squares. The db_bin() function is used for each continuous variable inside group_by(), but in this case the number if bins is increased to 50:\n\ndf <- tbl(con, \"flights\") %>%\n  group_by(\n    sc_dep_time = !! db_bin(sched_dep_time, bins = 50),\n    sc_arr_time = !! db_bin(sched_arr_time, bins = 50)\n  ) %>%\n  summarise(avg_distance = mean(distance)) %>%\n  collect()\n\nhead(df)\n\n\n\nPlotting results in R\nThe plot can now be built using geom_raster(). Assigning x and y to each of the continuous variables will depend on what makes more sense for a given visualization. The result of each intersection is passed as the color of the square using fill.\n\nggplot(data = df) +\n  geom_raster(aes(x = sc_dep_time, y = sc_arr_time, fill = avg_distance))\n\n\n\nConsiderations\nThere are two considerations when using a Raster plot with a database. Both considerations are related to the size of the results downloaded from the database:\n\nThe number of bins requested: The higher the bins value is, the more data is downloaded from the database.\nHow concentrated the data is: This refers to how many intersections return a value. The more intersections without a value, the less data is downloaded from the database.\n\nIn the previous example, there is a maximum of 2,500 rows (50 x 50). Because the data is highly concentrated, only 353 records are returned. This means that the data will be transmitted over the network quickly, but the trade-off is that the picture definition may not be ideal to gain insights about the data.\nIn the following example, the “definition” is set at 100 x 100. This improves the resolution but it quadruples the number of records that could potentially be downloaded.\n\ntbl(con, \"flights\") %>%\n  group_by(\n    sc_dep_time = !! db_bin(sched_dep_time, bins = 100),\n    sc_arr_time = !! db_bin(sched_arr_time, bins = 100)\n  ) %>%\n  summarise(avg_distance = mean(distance)) %>%\n  collect() %>%\n  ggplot() +\n   geom_raster(aes(x = sc_dep_time, y = sc_arr_time, fill = avg_distance))"
  },
  {
    "href": "best-practices/visualization.html#use-an-r-package",
    "title": "Creating Visualizations",
    "section": "Use an R package",
    "text": "The dbplot package provides helper functions that automate the aggregation and plotting steps. For more info, visit the dbplot article in this website."
  },
  {
    "href": "best-practices/schema.html#overview",
    "title": "Schema selection",
    "section": "Overview",
    "text": "It is common for enterprise databases to use multiple schemata to partition the data, it is either separated by business domain or some other context.\nThis is especially true for Data warehouses. It is rare when the default schema is going to have all of the data needed for an analysis.\nFor analyses using dplyr, the in_schema() function should cover most of the cases when the non-default schema needs to be accessed."
  },
  {
    "href": "best-practices/schema.html#an-example",
    "title": "Schema selection",
    "section": "An example",
    "text": "The following ODBC connection opens a session with the datawarehouse database: ::: {.cell layout.align=“center” hash=“schema_cache/html/unnamed-chunk-4_75ea7abc417942d59698d1dc66c3ef2a”}\ncon <- DBI::dbConnect(odbc::odbc(), \"datawarehouse\")\n:::\nThe database contains several schemata. The default schema is dbo. So to it is very straightforward to access it via dplyr. The difficulty occurs when attempting to access a table not in that schema, such as tables in the production schema.\n\nThis is how to access a table inside the dbo schema, using dplyr: ::: {.cell layout.align=“center” hash=“schema_cache/html/unnamed-chunk-6_e5c6d1d48afac0efc52a229caaa46e1b”}\nlibrary(dplyr)\nlibrary(dbplyr)\n\ntbl(con, \"mtcars\") %>%\n  head()\n:::\nThe same approach does not work for accessing the flights table, which resides in the production schema: ::: {.cell layout.align=“center” hash=“schema_cache/html/unnamed-chunk-8_7ff3c58886cf23996133585556163b96”}\ntbl(con, \"flights\")\n:::\nError: <SQL> 'SELECT * FROM \"flights\" AS \"zzz2\" WHERE (0 = 1)' nanodbc/nanodbc.cpp:\n1587: 42S02: [Microsoft][ODBC SQL Server Driver][SQL Server]Invalid object name \n'flights'."
  },
  {
    "href": "best-practices/schema.html#using-in_schema",
    "title": "Schema selection",
    "section": "Using in_schema()",
    "text": "The in_schema() function works by passing it inside the tbl() function. The schema and table are passed as quoted names: ::: {.cell layout.align=“center” hash=“schema_cache/html/unnamed-chunk-10_f49b6534812c6b2c71067a825f11d2eb”}\ntbl(con, in_schema(\"production\", \"flights\")) %>%\n  head()\n:::\n\nIdeal use\nFor interactive use, we would avoid using the tbl() command at the top of every dplyr piped code set. So it is better to load the table pointer into a variable: ::: {.cell layout.align=“center” hash=“schema_cache/html/unnamed-chunk-12_42206cf65061f778cfd48a0a4fd24ee6”}\ndb_flights <- tbl(con, in_schema(\"production\", \"flights\"))\n:::\nAn additional advantage of loading a variable with the table reference is that the field auto-completion is activated. This happens because the vars attribute, from the tbl() output, is loaded in the variable.\nThe operations that follow become more natural for a dplyr user ::: {.cell layout.align=“center” hash=“schema_cache/html/unnamed-chunk-14_77fd6af6a6dd00a6c3d1063cee06be67”}\ndb_flights %>%\n  group_by(month) %>%\n  summarise(\n    canceled= sum(cancelled, na.rm = TRUE),\n    total = n()) %>%\n  arrange(month)\n:::"
  },
  {
    "href": "best-practices/schema.html#writing-data",
    "title": "Schema selection",
    "section": "Writing data",
    "text": "The copy_to() command defaults to creating and populating temporary tables. So when used with in_schema(), the most likely result is that the command will be ignored, and a table called “[schema].[table]” is created. ::: {.cell layout.align=“center” hash=“schema_cache/html/unnamed-chunk-16_715306886d11183146be954d2ce49b5f”}\ncopy_to(con, iris, in_schema(\"production\", \"iris\"))\n:::\nCreated a temporary table named: ##production.iris\nEach enterprise grade database has its own way to manage of temporary tables. So the best course of action is to relay on the those mechanisms, and just request a temporary table.\n\ndb_iris <- copy_to(con, iris)\n\nhead(db_iris)\n\nIn this particular case, the iris dataset was copied to the tempdb database, but in a mirror schema called production\n\nWrite non-temporary tables\nThe best way to create a permanent table, inside a specific schema, is to use the DBI package. The dbWriteTable() and SQL() commands should accomplish the task:\n\nlibrary(DBI)\n\ndbWriteTable(con, SQL(\"production.iris\"), iris)"
  },
  {
    "href": "best-practices/run-queries-safely.html#sql-injection-attack",
    "title": "Run Queries Safely",
    "section": "SQL Injection Attack",
    "text": "The dbGetQuery() command allows us to write queries and retrieve the results. The query has to be written using the SQL syntax that matches to the database type.\nFor example, here is a database that contains the airports data from NYC Flights data:\n\ndbGetQuery(con, \"SELECT * FROM airports LIMIT 5\")\n\n  faa                          name      lat       lon  alt tz dst\n1 04G             Lansdowne Airport 41.13047 -80.61958 1044 -5   A\n2 06A Moton Field Municipal Airport 32.46057 -85.68003  264 -6   A\n3 06C           Schaumburg Regional 41.98934 -88.10124  801 -6   A\n4 06N               Randall Airport 41.43191 -74.39156  523 -5   A\n5 09J         Jekyll Island Airport 31.07447 -81.42778   11 -5   A\n\n\nOften you need to write queries that depend on user input. For example, you might want to allow the user to pick an airport to focus their analysis on. To do this, it’s tempting to create the SQL string yourself by pasting strings together:\n\nairport_code <- \"GPT\"\ndbGetQuery(con, paste0(\"SELECT * FROM airports WHERE faa = '\", airport_code ,\"'\"))\n\n  faa            name      lat       lon alt tz dst\n1 GPT Gulfport-Biloxi 30.40728 -89.07011  28 -6   A\n\n\nHere airport_code is created in the script, in real-life it might be an input typed into a Shiny app.\nThe problem with creating SQL strings with paste0() is that a careful attacker can create inputs that return more rows than you want:\n\nairport_code <- \"GPT' or faa = 'MSY\"\ndbGetQuery(con, paste0(\"SELECT * FROM airports WHERE faa = '\", airport_code ,\"'\"))\n\n  faa                             name      lat       lon alt tz dst\n1 GPT                  Gulfport-Biloxi 30.40728 -89.07011  28 -6   A\n2 MSY Louis Armstrong New Orleans Intl 29.99339 -90.25803   4 -6   A\n\n\nOr take destructive actions on your database:\n\nairport_code <- \"GPT'; DROP TABLE 'airports\"\ndbGetQuery(con, paste0(\"SELECT * FROM airports WHERE faa = '\", airport_code ,\"'\"))\n\nThis is called SQL injection attack.\nThere are three ways to avoid this problem:\n\nUse a parameterised query with dbSendQuery() and dbBind()\nUse the sqlInterpolate() function to safely combine a SQL string with data\nManually escape the inputs using dbQuoteString()\n\nThese are ordered by the level of safety they provide: if you can use dbSendQuery() and dbBind(), you should."
  },
  {
    "href": "best-practices/run-queries-safely.html#parameterized-queries",
    "title": "Run Queries Safely",
    "section": "Parameterized queries",
    "text": "All modern database engines provide a way to write parameterised queries, queries that contain some placeholder that allows you to re-run the query multiple times with different inputs. This protects you from SQL injection attacks, and as an added benefit, the database can often optimise the query so it runs faster.\nUsing a parameterised query with DBI requires three steps.\n\nYou create a query containing a ? placeholder and send it to the database with dbSendQuery():\n\n::: {.cell indent=” “}\n```{.r .cell-code}\nairport <- dbSendQuery(con, \"SELECT * FROM airports WHERE faa = ?\")\n```\n:::\n\nUse dbBind() to execute the query with specific values, then dbFetch() to get the results:\n\n::: {.cell indent=” “}\n```{.r .cell-code}\ndbBind(airport, list(\"GPT\"))\ndbFetch(airport)\n```\n\n::: {.cell-output-stdout}\n```\n  faa            name      lat       lon alt tz dst\n1 GPT Gulfport-Biloxi 30.40728 -89.07011  28 -6   A\n```\n:::\n:::\n\nOnce you’re done using the parameterised query, clean it up by calling dbClearResult()\n\n::: {.cell indent=” “}\n```{.r .cell-code}\ndbClearResult(airport)\n```\n:::"
  },
  {
    "href": "best-practices/run-queries-safely.html#using-glue_sql",
    "title": "Run Queries Safely",
    "section": "Using glue_sql()",
    "text": "Parameterized queries are generally the safest and most efficient way to pass user defined values in a query, however not every database driver supports them. The function glue_sql(), part of the the glue package, is able to handle the SQL quoting and variable placement.\n\nlibrary(glue)\n\nairport_sql <- glue_sql(\"SELECT * FROM airports WHERE faa = ?\")\nairport <- dbSendQuery(con, airport_sql)\n\ndbBind(airport, list(\"GPT\"))\ndbFetch(airport)\n\n  faa            name      lat       lon alt tz dst\n1 GPT Gulfport-Biloxi 30.40728 -89.07011  28 -6   A\n\ndbClearResult(airport)\n\nIf you place an astersk * at the end of a glue expression the values will be collapsed with commas. This is useful for the SQL IN Operator for instance.\n\nairport_sql <- glue_sql(\"SELECT * FROM airports WHERE faa IN ({airports*})\", \n                        airports = c(\"GPT\", \"MSY\"),\n                        .con = con\n                        )\n\nairport <- dbSendQuery(con, airport_sql)\n\ndbFetch(airport)\n\n  faa                             name      lat       lon alt tz dst\n1 GPT                  Gulfport-Biloxi 30.40728 -89.07011  28 -6   A\n2 MSY Louis Armstrong New Orleans Intl 29.99339 -90.25803   4 -6   A\n\ndbClearResult(airport)"
  },
  {
    "href": "best-practices/run-queries-safely.html#interpolation-by-hand",
    "title": "Run Queries Safely",
    "section": "Interpolation by “hand”",
    "text": "While all modern databases support parameterised queries, they are not always supported in individual database drivers. If you find that dbBind() doesn’t work with the database connector you are using, you can fall back on sqlInterpolate(), which will safely do the interpolation for you.\n\nairport_code <- \"GPT\"\n\nsql <- sqlInterpolate(con, \n  \"SELECT * FROM airports  where faa = ?code\", \n  code = airport_code\n)\nsql\n\n<SQL> SELECT * FROM airports  where faa = 'GPT'\n\ndbGetQuery(con, sql)\n\n  faa            name      lat       lon alt tz dst\n1 GPT Gulfport-Biloxi 30.40728 -89.07011  28 -6   A\n\n\nThe query returns no records if we try the same SQL injection attack:\n\nairport_code <- \"GPT' or faa = 'MSY\"\n\nsql <- sqlInterpolate(con, \n  \"SELECT * FROM airports  where faa = ?code\", \n  code = airport_code\n)\nsql\n\n<SQL> SELECT * FROM airports  where faa = 'GPT'' or faa = ''MSY'\n\ndbGetQuery(con, sql)\n\n[1] faa  name lat  lon  alt  tz   dst \n<0 rows> (or 0-length row.names)"
  },
  {
    "href": "best-practices/run-queries-safely.html#manual-escaping",
    "title": "Run Queries Safely",
    "section": "Manual escaping",
    "text": "Sometimes you can’t create the SQL you want using either of the previous methods. If you’re in this unhappy situation, first make absolutely sure that you haven’t missed an existing DBI helper function that does what you need. You need to be extremely careful when doing the escaping yourself, and it’s better to rely on existing code that multiple people have carefully reviewed.\nHowever, if there’s no other way around it, you can use dbQuoteString() to add the quotes for you. This method will automatically take care of dangerous characters in the same way as sqlInterpolate() (better) and dbBind() (best).\n\nairport_code <- \"GPT' or faa = 'MSY\"\n\nsql <- paste0(\"SELECT * FROM airports WHERE faa = \", dbQuoteString(con, airport_code))\n\nsql\n\n[1] \"SELECT * FROM airports WHERE faa = 'GPT'' or faa = ''MSY'\"\n\ndbGetQuery(con, sql)\n\n[1] faa  name lat  lon  alt  tz   dst \n<0 rows> (or 0-length row.names)\n\n\nYou may also need dbQuoteIdentifier() if you are creating tables or relying on user input to choose which column to filter on."
  },
  {
    "href": "best-practices/deployment.html#service-account",
    "title": "Securing Deployed Content",
    "section": "Service Account",
    "text": "It is typical for shiny applications and R Markdown reports to provide insight from data that is not directly accessible by the content audience. In these 1-to-many cases, it is common to define service accounts that access the database on behalf of the content audience. The previous examples assumed this type of model.\nSometimes, during local development, the data scientist might be expected to use their own credentials. It is possible through a DSN or the config package to specify that local connections use the data scientist’s credentials and deployed connections use a service account. Be sure the code works for results for both accounts!"
  },
  {
    "href": "best-practices/deployment.html#query-by-user-shiny",
    "title": "Securing Deployed Content",
    "section": "Query by User (Shiny)",
    "text": "Even when a service account is used, it is still possible to restrict access to data using logic inside the application code. One option is to update the query based on the logged-in user. The username is available in Shiny applications through the session$user object. For example:\nlibrary(shiny)\nlibrary(DBI)\nlibrary(odbc)\n\ncon <- dbConnect(odbc::odbc(), \"Datawarehouse\")\n\n# ... rest of shiny code \n\nserver <- function(input, output, session) {\n   data <- reactive({\n       q <- \"SELECT * FROM sales-data WHERE user == ?user\"\n       query <- sqlInterpolate(con, q, user = session$user)\n       sqlGetQuery(con, query)\n   })\n   \n   # ... some code that uses data()\n}"
  },
  {
    "href": "best-practices/deployment.html#prompt-for-credentials-shiny",
    "title": "Securing Deployed Content",
    "section": "Prompt for Credentials (Shiny)",
    "text": "In Shiny it is also possible to prompt the user for their database credentials. For example:\nlibrary(shiny)\nlibrary(DBI)\nlibrary(odbc)\n\n# ... rest of shiny code \n\nserver <- function(input, output, session) {\n   data <- reactive({\n      req(input$uid, input$pwd)\n      con <- dbConnect(odbc::odbc(),\n        Driver = \"Postgres\",\n        Server = \"mydb.company.com\",\n        Port = 5432,\n        Database = \"sales-data\",\n        UID = input$uid,\n        PWD = input$pwd,\n      )\n      \n      data <- dbGetQuery(con, \"SELECT * FROM sales\")\n      \n      dbDisconnect(con)\n   \n   })\n   \n   # ... some code that uses data()\n}"
  },
  {
    "href": "best-practices/deployment.html#run-as-the-logged-in-user-kerberos",
    "title": "Securing Deployed Content",
    "section": "Run As the Logged-in User (Kerberos)",
    "text": "In rare cases, it may be necessary for the data to be accessed by the application or report on behalf of the specific logged-in user without prompting the user for their credentials.\nThis scenario is rare because it implies that each end user of the report or application has an account and access controls in the database. In other words, this model assumes a 1-to-1 model instead of the 1-to-many distribution model facilitated by a service account.\nIn these scenarios, it is most common to use Kerberos. RStudio Connect will need to be setup to run the application as the logged-in user. The admin guide contains more details.\nDeployment of this type of content is usually straightforward because the connection code does not include any credentials, and is the same in the local and deployed context.\nFor example:\nlibrary(DBI)\nlibrary(odbc)\n\ncon <- dbConnect(odbc::odbc(),\n  Driver = \"SQLServer\",\n  Database  = \"Datawarehouse\",\n  trusted_connection = \"True\"\n)\nFor more information on data access, see this article. In all cases, the credentials should not be stored as plain text in either the configuration file or the R code. See securing credentials for more details."
  },
  {
    "href": "best-practices/deployment.html#deploying-with-the-config-package",
    "title": "Securing Deployed Content",
    "section": "Deploying with the config Package",
    "text": "An alternative to relying on DSNs is to use the config package. The config package allows the connection code in R to reference an external file that defines values based on the environment. This process makes it easy to specify values to use for a connection locally and values to use after deployment.\nFor example:\nR code:\nlibrary(DBI)\nlibrary(odbc)\nlibrary(config)\n\ndw <- get(\"datawarehouse\")\n\ncon <- dbConnect(\n   Driver = dw$driver,\n   Server = dw$server,\n   UID    = dw$uid,\n   PWD    = dw$pwd,\n   Port   = dw$port,\n   Database = dw$database\n)\nconfig.yml:\ndefault:\n  datawarehouse:\n    driver: 'Postgres' \n    server: 'mydb-test.company.com'\n    uid: 'local-account'\n    pwd: 'my-password'  // not recommended, see alternatives below\n    port: 5432\n    database: 'regional-sales-sample'\n    \nrsconnect:\n  datawarehouse:\n    driver: 'PostgresPro'\n    server: 'mydb-prod.company.com'\n    uid: 'service-account'\n    pwd: 'service-password' // not recommended, see alternatives below\n    port: 5432\n    database: 'regional-sales-full'\nThe config package determines the active configuration by looking at the R_CONFIG_ACTIVE environment variable. By default, RStudio Connect sets R_CONFIG_ACTIVE to the value rsconnect. In the config file above, the default datawarehouse values would be used locally and the datawarehouse values defined in the rsconnect section would be used on RStudio Connect. Administrators can optionally customize the name of the active configuration used in Connect."
  },
  {
    "href": "best-practices/deployment.html#credentials-inside-environment-variables-in-rstudio-connect",
    "title": "Securing Deployed Content",
    "section": "Credentials inside Environment Variables in RStudio Connect",
    "text": "Starting with version 1.6, RStudio Connect allows R Environment Variables to be saved at the application level. The variables are encrypted on-disk, and in-memory.\nThe recommended approach would be to use an .Renviron file in your local session of R, which can be used to store the credentials, and then retrieved with Sys.getenv(). Here are the steps:\n\nCreate a new file defining the credentials:\n\n::: {.cell indent=” “}\n```{.r .cell-code}\nuserid = \"username\"\npwd = \"password\"\n```\n::: 2. Save it in your home directory with the file name .Renviron. If you are asked whether you want to save a file whose name begins with a dot, say YES.\n\nNote that by default, dot files are usually hidden. However, within RStudio, the file browser will make .Renviron visible and therefore easy to edit in the future.\nRestart R. .Renviron is processed only at the start of an R session.\nRetrieve the credentials using Sys.getenv() while opening the connection: ::: {.cell indent=” “}\ncon <- DBI::dbConnect(odbc::odbc(),\n  Driver = \"impala\", \n  Host   = \"database.rstudio.com\",\n  UID    = Sys.getenv(\"userid\"),\n  PWD    = Sys.getenv(\"pwd\")\n)\n:::\nDevelop the app or document and deploy to RStudio Connect. Make sure to leave the .Renviron file out of the deployment process.\nIn RStudio Connect, select the {X} Vars tab\nClick on Add Environment Variable blue button\nCreate each the password and user ID variables"
  }
]